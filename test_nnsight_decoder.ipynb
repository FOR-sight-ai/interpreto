{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight\n",
    "\n",
    "\n",
    "class TransformerDecoderWrapper(NNsight):\n",
    "    def get_latent_activations(self, sentences, chosen_layers):\n",
    "        \"\"\"\n",
    "        Extracts latent activations from specified layers.\n",
    "\n",
    "        Args:\n",
    "            sentences (list of str): List of input sentences.\n",
    "            chosen_layers (int or list of int): Layer(s) from which to extract activations.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of extracted activations {layer_name: activation}.\n",
    "        \"\"\"\n",
    "        if isinstance(chosen_layers, int):\n",
    "            chosen_layers = [chosen_layers]\n",
    "\n",
    "        layers_out = {}\n",
    "        with self.trace(sentences):\n",
    "            for layer in chosen_layers:\n",
    "                layers_out[f\"layer{layer}\"] = self.transformer.h[layer].output[0].save()\n",
    "\n",
    "            # Stop tracing at the last chosen layer\n",
    "            self.transformer.h[max(chosen_layers)].output.stop()\n",
    "\n",
    "        return layers_out\n",
    "\n",
    "    def run_end_model_from_layer(self, layer_activations, start_layer):\n",
    "        \"\"\"\n",
    "        Runs the model from a specific layer onwards until the final logits.\n",
    "\n",
    "        Args:\n",
    "            layer_activations (dict): Dictionary of latent activations from `get_latent_activations`.\n",
    "            start_layer (int): The layer from which to resume forward propagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Logits from the model.\n",
    "        \"\"\"\n",
    "        if f\"layer{start_layer}\" not in layer_activations:\n",
    "            raise ValueError(f\"Layer {start_layer} activations not found.\")\n",
    "\n",
    "        hidden_state = layer_activations[f\"layer{start_layer}\"]\n",
    "\n",
    "        for i, layer in enumerate(self.transformer.h[(start_layer + 1) :]):\n",
    "            hidden_state = layer(hidden_state) if i == 0 else layer(hidden_state[0])\n",
    "\n",
    "        logits = self.lm_head(hidden_state[0])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoderWrapper(\"openai-community/gpt2\", device_map=\"auto\")\n",
    "\n",
    "\n",
    "layers_out = model.get_latent_activations(\n",
    "    [\"Hello, how are you?\", \"Nice to meet you!\", \"Yes it can be cool.\", \"Ok nice, i can't wait!!\"], [2, 3, 5]\n",
    ")\n",
    "\n",
    "logits = model.run_end_model_from_layer(layers_out, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nnsight import NNsight\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model_hug = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "model = NNsight(model_hug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nnsight.intervention.contexts.interleaving.InterleavingTracer at 0x7fee3d2b2890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trace(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NNsightError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/tracing/graph/node.py\", line 289, in execute",
      "    self.target.execute(self)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py\", line 159, in execute",
      "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/intervention/base.py\", line 341, in interleave",
      "    with interleaver:",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/intervention/interleaver.py\", line 125, in __exit__",
      "    raise exc_val",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/intervention/base.py\", line 342, in interleave",
      "    return fn(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/nnsight/intervention/base.py\", line 452, in _execute",
      "    return self._model(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1603, in _call_impl",
      "    result = forward_call(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1271, in forward",
      "    transformer_outputs = self.transformer(",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl",
      "    return self._call_impl(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1603, in _call_impl",
      "    result = forward_call(*args, **kwargs)",
      "  File \"/home/fanny.jourdan/dev/interpreto/.venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1006, in forward",
      "    input_shape = input_ids.size()",
      "AttributeError: 'list' object has no attribute 'size'",
      "",
      "During handling of the above exception, another exception occurred:",
      "",
      "Traceback (most recent call last):",
      "  File \"/tmp/ipykernel_2997930/2613433433.py\", line 7, in <module>",
      "    with model.trace(sentence):",
      "",
      "NNsightError: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "sentence = [\"The quick brown fox jumps over the lazy dog.\"]  # , \"The dog is lazy.\"]\n",
    "chosen_layers = [0, 1, 2]\n",
    "\n",
    "\n",
    "layers_out = {}\n",
    "\n",
    "with model.trace(sentence):\n",
    "    for layer in chosen_layers:\n",
    "        layers_out[f\"layer{layer}\"] = model.transformer.h[layer].output[0].save()\n",
    "\n",
    "    # Stop tracing at the last chosen layer\n",
    "    model.transformer.h[max(chosen_layers)].output.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nnsight import LanguageModel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class TransformerDecoderWrapper(LanguageModel):\n",
    "    def get_latent_activations(self, sentences, chosen_layers, batch_size=4, padding_value=0):\n",
    "        \"\"\"\n",
    "        Extrait les activations latentes des couches spécifiées par batch et\n",
    "        retourne, pour chaque couche, un unique tensor paddé regroupant tous les batchs.\n",
    "\n",
    "        Args:\n",
    "            sentences (list of str): Liste de phrases en entrée.\n",
    "            chosen_layers (int ou list of int): Couches dont on souhaite extraire les activations.\n",
    "            batch_size (int): Taille de batch (par défaut 4).\n",
    "            padding_value (int ou float): Valeur de padding utilisée pour compléter les tenseurs.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionnaire des activations extraites {nom_de_la_couche: tensor paddé}.\n",
    "        \"\"\"\n",
    "        if isinstance(chosen_layers, int):\n",
    "            chosen_layers = [chosen_layers]\n",
    "\n",
    "        # Stockage temporaire sous forme de listes\n",
    "        layers_out = {f\"layer{layer}\": [] for layer in chosen_layers}\n",
    "\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i : i + batch_size]\n",
    "            with self.trace(batch):\n",
    "                for layer in chosen_layers:\n",
    "                    activation = self.transformer.h[layer].output[0].save()\n",
    "                    layers_out[f\"layer{layer}\"].append(activation)\n",
    "                self.transformer.h[max(chosen_layers)].output.stop()\n",
    "\n",
    "        # Padding et concaténation de tous les batchs pour chaque couche\n",
    "        for key in layers_out:\n",
    "            layers_out[key] = pad_sequence(layers_out[key], batch_first=True, padding_value=padding_value)\n",
    "\n",
    "        return layers_out\n",
    "\n",
    "    def run_end_model_from_layer(self, layer_activations, start_layer, batch_size=4):\n",
    "        \"\"\"\n",
    "        Exécute le modèle à partir d'une couche spécifique jusqu'aux logits finaux,\n",
    "        en traitant les activations latentes par batch.\n",
    "\n",
    "        Note : Ici, on suppose que `layer_activations[f\"layer{start_layer}\"]` est un tensor unique\n",
    "        obtenu après padding de tous les batchs.\n",
    "\n",
    "        Args:\n",
    "            layer_activations (dict): Dictionnaire des activations latentes.\n",
    "            start_layer (int): La couche à partir de laquelle reprendre la propagation.\n",
    "            batch_size (int): Taille de batch (par défaut 4).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Logits du modèle, éventuellement recomposés à partir des batchs.\n",
    "        \"\"\"\n",
    "        key = f\"layer{start_layer}\"\n",
    "        if key not in layer_activations:\n",
    "            raise ValueError(f\"Activations de la couche {start_layer} non trouvées.\")\n",
    "\n",
    "        hidden_state = layer_activations[key]\n",
    "        logits_batches = []\n",
    "\n",
    "        for i in range(0, hidden_state.shape[0], batch_size):\n",
    "            batch = hidden_state[i : i + batch_size]\n",
    "            out = batch\n",
    "            for j, layer in enumerate(self.transformer.h[(start_layer + 1) :]):\n",
    "                if j == 0:\n",
    "                    out = layer(out)\n",
    "                else:\n",
    "                    out = layer(out[0])\n",
    "            logits = self.lm_head(out[0])\n",
    "            logits_batches.append(logits)\n",
    "\n",
    "        # Vous pouvez concaténer les logits si les dimensions le permettent\n",
    "        return torch.cat(logits_batches, dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
