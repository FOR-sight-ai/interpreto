name: build

on:
  push:
    branches:
      - main
      - dev
  pull_request:

jobs:
  build:
    runs-on: ubuntu-latest
    if: github.actor != 'dependabot[bot]' && github.actor != 'dependabot-preview[bot]'
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4.3.0
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install uv
      run: |
        make uv-download

    - name: Set up cache
      uses: actions/cache@v4.2.2
      with:
        path: .venv
        key: venv-${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}

    - name: Install dependencies
      run: |
        make install-ci
        source .venv/bin/activate

    - name: Export HF_HUB_TOKEN
      run: echo "HF_HUB_TOKEN=${{ secrets.HUGGINGFACE_TOKEN }}" >> $GITHUB_ENV

    # Cache all tiny-random models in the github actions runner
    - name: Pre-cache all tiny-random models
        run: |
          source .venv/bin/activate
          python - <<'EOF'
          from transformers import (
              AutoModelForSequenceClassification,
              AutoModelForCausalLM,
              AutoModelForMaskedLM,
              AutoTokenizer,
          )

          # List all of the hf-internal-testing model IDs that your tests need:
          model_ids = [
              # conftest.py:
              "hf-internal-testing/tiny-random-bert",
              "hf-internal-testing/tiny-random-distilbert",
              "hf-internal-testing/tiny-random-gpt2",
              # parametrized test:
              "hf-internal-testing/tiny-random-albert",
              "hf-internal-testing/tiny-random-bart",
              "hf-internal-testing/tiny-random-DebertaV2Model",
              "hf-internal-testing/tiny-random-distilbert",   # duplicate allowed
              "hf-internal-testing/tiny-random-ElectraModel",
              "hf-internal-testing/tiny-random-roberta",
              "hf-internal-testing/tiny-random-t5",
              "hf-internal-testing/tiny-xlm-roberta",
              "hf-internal-testing/tiny-random-gpt2",
              "hf-internal-testing/tiny-random-gpt_neo",
              "hf-internal-testing/tiny-random-gptj",
              "hf-internal-testing/tiny-random-CodeGenForCausalLM",
              "hf-internal-testing/tiny-random-FalconModel",
              "hf-internal-testing/tiny-random-LlamaForCausalLM",
              "hf-internal-testing/tiny-random-MistralForCausalLM",
              "hf-internal-testing/tiny-random-Starcoder2ForCausalLM",
          ]

          # We will call .from_pretrained(...) once per model_id, 
          # and also download its tokenizer.  Because HF_HUB_TOKEN 
          # is set in the environment, these calls are authenticated 
          # and will not exhaust the low unauthenticated rate limit.
          for m in set(model_ids):  
              # Sequence classification or masked-LM vs causal-LM 
              # is determined by your conftest/test code. 
              # We can try all possible AutoModel types here, but 
              # since your conftest already splits them out, we just do:
              try:
                  AutoModelForSequenceClassification.from_pretrained(m)
              except Exception:
                  try:
                      AutoModelForMaskedLM.from_pretrained(m)
                  except Exception:
                      try:
                          AutoModelForCausalLM.from_pretrained(m)
                      except Exception as ee:
                          print(f"‐‐ failed to load model {m}: {ee}")
                          exit(1)
              # In any case, also grab the tokenizer:
              AutoTokenizer.from_pretrained(m)
              print(f"✔ cached {m}")
          EOF

    - name: Run style checks
      run: |
        make lint

    - name: Run fast tests
      run: |
        make fast-test-ci
