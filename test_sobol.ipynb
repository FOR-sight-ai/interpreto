{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 15 Sequence length: 17\n",
      "Sobol attribution indices: [0.00215907 0.0124188  0.0163542  0.02872035 0.03049477 0.15114236\n",
      " 0.02000404 0.05688819 0.00708974 0.01939227 0.00886717 0.04474352\n",
      " 0.00648187 0.0184624  0.01511224 0.0068806  0.19568759]\n"
     ]
    }
   ],
   "source": [
    "###CODE FOR ALL TOKENS\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class SobolPerturbator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, baseline=\"[MASK]\", n_perturbations=1000, proba=0.5):\n",
    "        \"\"\"\n",
    "        - tokenizer: Hugging Face tokenizer associated with the model\n",
    "        - baseline: replacement token (e.g. “[MASK]”)\n",
    "        - n_perturbations: number of Monte Carlo samples\n",
    "        - proba: probability of keeping a token (i.e. putting 1 in the mask)\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.baseline = baseline\n",
    "        self.n_perturbations = n_perturbations\n",
    "        self.proba = proba\n",
    "\n",
    "    def perturb(self, text, Sobol_indices=\"first order\"):\n",
    "        \"\"\"\n",
    "        Generates a mask matrix of size (n_perturbations x seq_len) and associated perturbations.\n",
    "\n",
    "        Parameters:\n",
    "          - text : original sentence (str)\n",
    "          - Sobol_indices : \"first order\" or \"total\" (str)\n",
    "\n",
    "        Returns a dictionary containing:\n",
    "          - \"origin perturbated inputs\": dictionary containing the input_ids and attention_mask for the original perturbations.\n",
    "          - \"list of perturbated inputs for each token\": list of dictionaries containing the input_ids and attention_mask\n",
    "            for the perturbations obtained by flipping each token on the original perturbations.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Tokenize text; note that tokenized tensors have shape (1, seq_len)\n",
    "        inputs_model = self.tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "        # Remove the extra batch dimension so that we have shape (seq_len,)\n",
    "        input_ids = inputs_model[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs_model[\"attention_mask\"].squeeze(0)\n",
    "        baseline_id = self.tokenizer.convert_tokens_to_ids(self.baseline)\n",
    "\n",
    "        seq_len = input_ids.shape[0]\n",
    "        print(\"Number of tokens:\", len(self.tokenizer.tokenize(text)), \"Sequence length:\", seq_len)\n",
    "\n",
    "        # Create the original perturbations (i.e. the original text with some tokens replaced by the baseline token)\n",
    "        origin_masks = []\n",
    "        origin_input_ids_list = []\n",
    "        origin_attention_mask_list = []\n",
    "        for _ in range(self.n_perturbations):\n",
    "            # Create a binary mask with probability self.proba for keeping the token\n",
    "            origin_mask = torch.bernoulli(torch.full((seq_len,), self.proba)).long()\n",
    "            # Replace dropped tokens with the baseline token\n",
    "            origin_input_ids = input_ids * origin_mask + baseline_id * (1 - origin_mask)\n",
    "            origin_masks.append(origin_mask)\n",
    "            origin_input_ids_list.append(origin_input_ids)\n",
    "            origin_attention_mask_list.append(attention_mask)\n",
    "        # Stack the perturbations: now each tensor is of shape (n_perturbations, seq_len)\n",
    "        origin_input_ids_tensor = torch.stack(origin_input_ids_list).to(device)\n",
    "        origin_attention_mask_tensor = torch.stack(origin_attention_mask_list).to(device)\n",
    "        origin_inputs_model = {\n",
    "            \"input_ids\": origin_input_ids_tensor,\n",
    "            \"attention_mask\": origin_attention_mask_tensor,\n",
    "        }\n",
    "\n",
    "        # For each token position, create perturbations by flipping that token's mask in the original perturbations\n",
    "        pert_inputs_model_per_token = []\n",
    "        for i in range(seq_len):\n",
    "            pert_input_ids_list = []\n",
    "            pert_attention_mask_list = []\n",
    "            for j in range(self.n_perturbations):\n",
    "                pert_mask = origin_masks[j].clone()\n",
    "                # Flip the i-th bit (if 1 then 0, if 0 then 1)\n",
    "                pert_mask[i] = 1 - pert_mask[i]\n",
    "                # If total Sobol indices are desired, flip all bits except the i-th bit\n",
    "                if Sobol_indices == \"total\":\n",
    "                    pert_mask = 1 - pert_mask\n",
    "                pert_input_ids = input_ids * pert_mask + baseline_id * (1 - pert_mask)\n",
    "                pert_input_ids_list.append(pert_input_ids)\n",
    "                pert_attention_mask_list.append(attention_mask)\n",
    "            # Stack each token's perturbations so that tensors have shape (n_perturbations, seq_len)\n",
    "            pert_inputs_model = {\n",
    "                \"input_ids\": torch.stack(pert_input_ids_list).to(device),\n",
    "                \"attention_mask\": torch.stack(pert_attention_mask_list).to(device),\n",
    "            }\n",
    "            pert_inputs_model_per_token.append(pert_inputs_model)\n",
    "\n",
    "        return {\n",
    "            \"origin perturbated inputs\": origin_inputs_model,\n",
    "            \"list of perturbated inputs for each token\": pert_inputs_model_per_token,\n",
    "        }\n",
    "\n",
    "\n",
    "def inference(model, inputs_model):\n",
    "    \"\"\"\n",
    "    Run the model on the inputs_model and return the logits.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_model)\n",
    "    logits = outputs.logits\n",
    "    return logits\n",
    "\n",
    "\n",
    "def batched_inference(model, inputs_model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Run the model on the inputs_model in batches and return the concatenated logits.\n",
    "\n",
    "    Parameters:\n",
    "      - model: the model to run inference on.\n",
    "      - inputs_model: dictionary with keys \"input_ids\" and \"attention_mask\", each of shape (n_samples, seq_len)\n",
    "      - batch_size: the batch size for inference.\n",
    "    \"\"\"\n",
    "    input_ids = inputs_model[\"input_ids\"]\n",
    "    attention_mask = inputs_model[\"attention_mask\"]\n",
    "    n_samples = input_ids.shape[0]\n",
    "    logits_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_input_ids = input_ids[i : i + batch_size]\n",
    "            batch_attention_mask = attention_mask[i : i + batch_size]\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits_list.append(outputs.logits)\n",
    "    logits = torch.cat(logits_list, dim=0)\n",
    "    return logits\n",
    "\n",
    "\n",
    "class SobolAggregator:\n",
    "    \"\"\"\n",
    "    Sobol indices aggregation\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate(f_orig, l_f_hybrid):\n",
    "        \"\"\"\n",
    "        Compute the Sobol indices from the output on the original perturbations (f_orig)\n",
    "        and the list of outputs for token-specific perturbations (l_f_hybrid).\n",
    "        The Sobol indices (first order or total) depend on the chosen perturbation scheme.\n",
    "        \"\"\"\n",
    "        # Convert to numpy if necessary\n",
    "        if torch.is_tensor(f_orig):\n",
    "            f_orig = f_orig.cpu().detach().numpy()\n",
    "\n",
    "        num_tokens = len(l_f_hybrid)\n",
    "        S = np.zeros(num_tokens)\n",
    "        var_f = np.var(f_orig)\n",
    "        # To avoid division by zero\n",
    "        if var_f == 0:\n",
    "            var_f = 1e-6\n",
    "\n",
    "        # Calculate the sensitivity index for each token\n",
    "        for i in range(num_tokens):\n",
    "            f_hybrid = l_f_hybrid[i]\n",
    "            if torch.is_tensor(f_hybrid):\n",
    "                f_hybrid = f_hybrid.cpu().detach().numpy()\n",
    "            delta = f_orig - f_hybrid\n",
    "            S[i] = np.mean(delta**2) / var_f\n",
    "\n",
    "        return S\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "text = \"This is an example sentence for Sobol attribution in NLP.\"\n",
    "baseline = \"[MASK]\"\n",
    "num_pert = 200  # Number of Monte Carlo samples\n",
    "p = 0.5\n",
    "\n",
    "perturbator = SobolPerturbator(tokenizer, baseline, num_pert, p)\n",
    "perturbation_outputs = perturbator.perturb(text, Sobol_indices=\"first order\")\n",
    "\n",
    "# Run inference on the original perturbations\n",
    "# f_orig = inference(model, inputs_model=perturbation_outputs[\"origin perturbated inputs\"])\n",
    "f_orig = batched_inference(model, inputs_model=perturbation_outputs[\"origin perturbated inputs\"], batch_size=32)\n",
    "\n",
    "# Run inference for token-specific perturbations for each token position\n",
    "l_f_hybrid = [\n",
    "    inference(model, inputs_model=perturbation_outputs[\"list of perturbated inputs for each token\"][i])\n",
    "    for i in range(len(perturbation_outputs[\"list of perturbated inputs for each token\"]))\n",
    "]\n",
    "\n",
    "aggregator = SobolAggregator()\n",
    "sobol_attribution = aggregator.aggregate(f_orig, l_f_hybrid)\n",
    "\n",
    "print(\"Sobol attribution indices:\", sobol_attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all tokens: 17 number of real tokens: 15 Sequence length: 17\n",
      "All tokens: ['[CLS]', 'this', 'is', 'an', 'example', 'sentence', 'for', 'sob', '##ol', 'at', '##tri', '##bution', 'in', 'nl', '##p', '.', '[SEP]']\n",
      "Real tokens: {1: 'this', 2: 'is', 3: 'an', 4: 'example', 5: 'sentence', 6: 'for', 7: 'sob', 8: '##ol', 9: 'at', 10: '##tri', 11: '##bution', 12: 'in', 13: 'nl', 14: '##p', 15: '.'}\n",
      "Sobol attribution indices for real tokens:\n",
      "Token 1: 'this' -> 0.012475554831326008\n",
      "Token 2: 'is' -> 0.01789218559861183\n",
      "Token 3: 'an' -> 0.02246691845357418\n",
      "Token 4: 'example' -> 0.030135653913021088\n",
      "Token 5: 'sentence' -> 0.1560068279504776\n",
      "Token 6: 'for' -> 0.021225513890385628\n",
      "Token 7: 'sob' -> 0.05978991463780403\n",
      "Token 8: '##ol' -> 0.010796360671520233\n",
      "Token 9: 'at' -> 0.019393419846892357\n",
      "Token 10: '##tri' -> 0.008098594844341278\n",
      "Token 11: '##bution' -> 0.04226751998066902\n",
      "Token 12: 'in' -> 0.00629765959456563\n",
      "Token 13: 'nl' -> 0.019798938184976578\n",
      "Token 14: '##p' -> 0.015558471903204918\n",
      "Token 15: '.' -> 0.0055503202602267265\n"
     ]
    }
   ],
   "source": [
    "###CODE FOR REAL TOKENS\n",
    "from __future__ import annotations  # noqa: F404\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "class SobolPerturbator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, baseline=\"[MASK]\", n_perturbations=1000, proba=0.5):\n",
    "        \"\"\"\n",
    "        - tokenizer: Hugging Face tokenizer associated with the model\n",
    "        - baseline: replacement token (e.g. “[MASK]”)\n",
    "        - n_perturbations: number of Monte Carlo samples\n",
    "        - proba: probability of keeping a token (i.e. putting 1 in the mask)\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.baseline = baseline\n",
    "        self.n_perturbations = n_perturbations\n",
    "        self.proba = proba\n",
    "\n",
    "    def perturb(self, text, Sobol_indices=\"first order\"):\n",
    "        \"\"\"\n",
    "        Generates perturbations for the entire input and for each \"real\" token position only.\n",
    "\n",
    "        Parameters:\n",
    "          - text : original sentence (str)\n",
    "          - Sobol_indices : \"first order\" or \"total\" (str)\n",
    "\n",
    "        Returns a dictionary containing:\n",
    "          - \"origin perturbated inputs\": dictionary with input_ids and attention_mask for the full-sequence perturbations.\n",
    "          - \"list of perturbated inputs for each token\": a dict mapping each real token's position to its own perturbation inputs.\n",
    "          - \"real_tokens\": a dict mapping each real token position to its token string.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Tokenize text with offsets (includes special tokens)\n",
    "        inputs_model = self.tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "        offset_mapping = (\n",
    "            inputs_model[\"offset_mapping\"].squeeze(0).tolist()\n",
    "        )  # List of (start, end) pairs for each token\n",
    "        input_ids = inputs_model[\"input_ids\"].squeeze(0)  # Shape: (seq_len,)\n",
    "        attention_mask = inputs_model[\"attention_mask\"].squeeze(0)  # Shape: (seq_len,)\n",
    "        all_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Identify \"real\" tokens: those with a nonzero span in the offset mapping.\n",
    "        real_indices = [i for i, (start, end) in enumerate(offset_mapping) if (end - start) > 0]\n",
    "        real_tokens = {i: all_tokens[i] for i in real_indices}\n",
    "\n",
    "        baseline_id = self.tokenizer.convert_tokens_to_ids(self.baseline)\n",
    "        seq_len = input_ids.shape[0]\n",
    "        print(\n",
    "            \"Number of all tokens:\",\n",
    "            len(all_tokens),\n",
    "            \"number of real tokens:\",\n",
    "            len(real_tokens),\n",
    "            \"Sequence length:\",\n",
    "            seq_len,\n",
    "        )\n",
    "        print(\"All tokens:\", all_tokens)\n",
    "        print(\"Real tokens:\", real_tokens)\n",
    "\n",
    "        # Create origin perturbations for the entire sequence.\n",
    "        origin_masks = []\n",
    "        origin_input_ids_list = []\n",
    "        origin_attention_mask_list = []\n",
    "        for _ in range(self.n_perturbations):\n",
    "            # Create a binary mask with probability self.proba for keeping the token.\n",
    "            origin_mask = torch.bernoulli(torch.full((seq_len,), self.proba)).long()\n",
    "            origin_input_ids = input_ids * origin_mask + baseline_id * (1 - origin_mask)\n",
    "            origin_masks.append(origin_mask)\n",
    "            origin_input_ids_list.append(origin_input_ids)\n",
    "            origin_attention_mask_list.append(attention_mask)\n",
    "        origin_input_ids_tensor = torch.stack(origin_input_ids_list).to(device)\n",
    "        origin_attention_mask_tensor = torch.stack(origin_attention_mask_list).to(device)\n",
    "        origin_inputs_model = {\n",
    "            \"input_ids\": origin_input_ids_tensor,\n",
    "            \"attention_mask\": origin_attention_mask_tensor,\n",
    "        }\n",
    "\n",
    "        # For each real token position, create perturbations by flipping that token's mask.\n",
    "        pert_inputs_model_per_token = {}\n",
    "        for i in real_indices:\n",
    "            pert_input_ids_list = []\n",
    "            pert_attention_mask_list = []\n",
    "            for j in range(self.n_perturbations):\n",
    "                pert_mask = origin_masks[j].clone()\n",
    "                # Flip the bit at token position i.\n",
    "                pert_mask[i] = 1 - pert_mask[i]\n",
    "                # If computing total Sobol indices, flip all bits except the i-th bit.\n",
    "                if Sobol_indices == \"total\":\n",
    "                    pert_mask = 1 - pert_mask\n",
    "                pert_input_ids = input_ids * pert_mask + baseline_id * (1 - pert_mask)\n",
    "                pert_input_ids_list.append(pert_input_ids)\n",
    "                pert_attention_mask_list.append(attention_mask)\n",
    "            pert_inputs_model = {\n",
    "                \"input_ids\": torch.stack(pert_input_ids_list).to(device),\n",
    "                \"attention_mask\": torch.stack(pert_attention_mask_list).to(device),\n",
    "            }\n",
    "            pert_inputs_model_per_token[i] = pert_inputs_model\n",
    "\n",
    "        return {\n",
    "            \"origin perturbated inputs\": origin_inputs_model,\n",
    "            \"list of perturbated inputs for each token\": pert_inputs_model_per_token,\n",
    "            \"real_tokens\": real_tokens,\n",
    "        }\n",
    "\n",
    "\n",
    "class SobolInferenceWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def inference(self, inputs_model):\n",
    "        \"\"\"\n",
    "        Run the model on the inputs_model and return the logits.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs_model)\n",
    "        return outputs.logits\n",
    "\n",
    "    def batched_inference(self, inputs_model, batch_size=32):\n",
    "        \"\"\"\n",
    "        Run the model on the inputs_model in batches and return the concatenated logits.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs_model: dictionary with keys \"input_ids\" and \"attention_mask\", each of shape (n_samples, seq_len)\n",
    "        - batch_size: the batch size for inference.\n",
    "        \"\"\"\n",
    "        input_ids = inputs_model[\"input_ids\"]\n",
    "        attention_mask = inputs_model[\"attention_mask\"]\n",
    "        n_samples = input_ids.shape[0]\n",
    "        logits_list = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_input_ids = input_ids[i : i + batch_size]\n",
    "                batch_attention_mask = attention_mask[i : i + batch_size]\n",
    "                outputs = self.model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "                logits_list.append(outputs.logits)\n",
    "        logits = torch.cat(logits_list, dim=0)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class SobolAggregator:\n",
    "    \"\"\"\n",
    "    Aggregates Sobol indices from model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate(f_orig, dict_f_hybrid):\n",
    "        \"\"\"\n",
    "        Compute the Sobol indices from the model outputs on the origin perturbations (f_orig)\n",
    "        and the token-specific perturbations (dict_f_hybrid).\n",
    "\n",
    "        Returns a dictionary mapping the token index to its Sobol attribution index.\n",
    "        \"\"\"\n",
    "        # Convert to numpy if necessary.\n",
    "        if torch.is_tensor(f_orig):\n",
    "            f_orig = f_orig.cpu().detach().numpy()\n",
    "        var_f = np.var(f_orig)\n",
    "        # To avoid division by zero.\n",
    "        if var_f == 0:\n",
    "            var_f = 1e-6\n",
    "        S = {}\n",
    "        for token_idx, f_hybrid in dict_f_hybrid.items():\n",
    "            if torch.is_tensor(f_hybrid):\n",
    "                f_hybrid = f_hybrid.cpu().detach().numpy()\n",
    "            delta = f_orig - f_hybrid\n",
    "            S[token_idx] = np.mean(delta**2) / var_f\n",
    "        return S\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "text = \"This is an example sentence for Sobol attribution in NLP.\"\n",
    "baseline = \"[MASK]\"\n",
    "num_pert = 200  # Number of Monte Carlo samples\n",
    "p = 0.5\n",
    "\n",
    "# Create perturbations.\n",
    "perturbator = SobolPerturbator(tokenizer, baseline, num_pert, p)\n",
    "perturbation_outputs = perturbator.perturb(text, Sobol_indices=\"first order\")\n",
    "\n",
    "# Run inference on the full-sequence perturbations.\n",
    "model_wrapper = SobolInferenceWrapper(model)\n",
    "# f_orig = model_wrapper.inference(inputs_model=perturbation_outputs[\"origin perturbated inputs\"])\n",
    "f_orig = model_wrapper.batched_inference(inputs_model=perturbation_outputs[\"origin perturbated inputs\"], batch_size=32)\n",
    "\n",
    "# Run inference for each token-specific perturbation (only for real tokens).\n",
    "l_f_hybrid = {}\n",
    "for token_idx, pert_inputs in perturbation_outputs[\"list of perturbated inputs for each token\"].items():\n",
    "    l_f_hybrid[token_idx] = model_wrapper.inference(inputs_model=pert_inputs)\n",
    "\n",
    "aggregator = SobolAggregator()\n",
    "sobol_attribution = aggregator.aggregate(f_orig, l_f_hybrid)\n",
    "\n",
    "# Display the Sobol attribution for each \"real\" token.\n",
    "real_tokens = perturbation_outputs[\"real_tokens\"]\n",
    "print(\"Sobol attribution indices for real tokens:\")\n",
    "for idx, attr in sobol_attribution.items():\n",
    "    print(f\"Token {idx}: '{real_tokens[idx]}' -> {attr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpreto",
   "language": "python",
   "name": "interpreto"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
