{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#table-of-contents","title":"\ud83d\udcda Table of contents","text":"<ul> <li>\ud83d\udcda Table of contents</li> <li>\ud83d\ude80 Quick Start</li> <li>\ud83d\udce6 What's Included</li> <li>\ud83d\udc4d Contributing</li> <li>\ud83d\udc40 See Also</li> <li>\ud83d\ude4f Acknowledgments</li> <li>\ud83d\udc68\u200d\ud83c\udf93 Creators</li> <li>\ud83d\uddde\ufe0f Citation</li> <li>\ud83d\udcdd License</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>The library hasn't been officially released yet, but if you'd like to try out its first features, just clone the dev branch on github and have a look at the examples notebooks.</p>"},{"location":"#whats-included","title":"\ud83d\udce6 What's Included","text":"<p>Interpreto \ud83e\ude84 provides a modular framework encompassing Attribution Methods, Concept-Based Methods, and Evaluation Metrics.</p>"},{"location":"#attribution-methods","title":"Attribution Methods","text":"Interpreto includes both inference-based and gradient-based attribution methods:  *We currently have these methods available:*  **Inference-based Methods:**  - Occlusion: [Zeiler and Fergus, 2014. Visualizing and understanding convolutional networks](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53). - LIME: [Ribeiro et al. 2013, \"Why should i trust you?\" explaining the predictions of any classifier](https://dl.acm.org/doi/abs/10.1145/2939672.2939778). - Kernel SHAP: [Lundberg and Lee, 2017, A Unified Approach to Interpreting Model Predictions](https://arxiv.org/abs/1705.07874). - Sobol Attribution: [Fel et al. 2021, Look at the variance! efficient black-box explanations with sobol-based sensitivity analysis](https://proceedings.neurips.cc/paper/2021/hash/da94cbeff56cfda50785df477941308b-Abstract.html).  **Gradient based methods:**  - Saliency: [Simonyan et al. 2013, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034). - Integrated Gradient: [Sundararajan et al. 2017, Axiomatic Attribution for Deep Networks](http://proceedings.mlr.press/v70/sundararajan17a.html). - SmoothGrad: [Smilkov et al. 2017, SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825)  *We will be adding these methods soon (Gradient based methods):*  - InputxGradient: [Simonyan et al. 2013, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034). - DeepLift: [Shrikumar et al. 2017, Learning Important Features Through Propagating Activation Differences](http://proceedings.mlr.press/v70/shrikumar17a). - VarGrad: [Richter et al. 2020, VarGrad: A Low-Variance Gradient Estimator for Variational Inference](https://proceedings.neurips.cc/paper/2020/hash/9c22c0b51b3202246463e986c7e205df-Abstract.html)"},{"location":"#concept-based-methods","title":"Concept-Based Methods","text":"Concept-based explanations aim to provide high-level interpretations of latent model representations.   Interpreto generalizes these methods through three core steps:  1. Concept Discovery (e.g., from latent embeddings) 2. Concept Interpretation (mapping discovered concepts to human-understandable elements) 3. Concept-to-Output Attribution (assessing concept relevance to model outputs)  **Concept Discovery Techniques** (via [Overcomplete](https://github.com/KempnerInstitute/overcomplete)):  - NMF, Semi-NMF, ConvexNMF - ICA, SVD, PCA - SAE variants (Vanilla SAE, TopK SAE, JumpReLU SAE, BatchTopK SAE)  **Available Concept Interpretation Techniques:**  - Top-k tokens from tokenizer vocabulary  *Concept Interpretation Techniques Added Soon:*  - Top-k tokens/words/clauses/sentences from specific datasets - Input-to-concept attribution from dataset examples ([Jourdan et al. 2023](https://aclanthology.org/2023.findings-acl.317/)) - Theme prediction via LLMs from top-k tokens/sentences  *Concept Interpretation Techniques Added Later:*  - OpenAI Interpretation ([Bills et al. 2023](https://openai.com/index/language-models-can-explain-neurons-in-language-models/)) - Aligning concepts with human labels ([Sajjad et al. 2022](https://aclanthology.org/2022.naacl-main.225/)) - Word cloud visualizations of concepts ([Dalvi et al. 2022](https://arxiv.org/abs/2205.07237)) - VocabProj &amp; TokenChange ([Gur-Arieh et al. 2025](https://arxiv.org/abs/2501.08319))  **Concept-to-Output Attribution:**  This part will be implemented later, but all the attribution methods presented above will be available here.  *Note that only methods with a concept extraction that has an encoder (input to concept) AND a decoder (concept to output) can use this function.*  **Specific methods:**  **[Available later when all parts are implemented]** Thanks to this generalization encompassing all concept-based methods and our highly flexible architecture, we can easily obtain a large number of concept-based methods:  - CAV and TCAV: [Kim et al. 2018, Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](http://proceedings.mlr.press/v80/kim18d.html) - ConceptSHAP: [Yeh et al. 2020, On Completeness-aware Concept-Based Explanations in Deep Neural Networks](https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html) - COCKATIEL: [Jourdan et al. 2023, COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP](https://aclanthology.org/2023.findings-acl.317/) - Yun et al. 2021, [Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors](https://arxiv.org/abs/2103.15949) - FFN values interpretation: [Geva et al. 2022, Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space](https://aclanthology.org/2022.emnlp-main.3/) - SparseCoding: [Cunningham et al. 2023, Sparse Autoencoders Find Highly Interpretable Features in Language Models](https://arxiv.org/abs/2309.08600) - Parameter Interpretation: [Dar et al. 2023, Analyzing Transformers in Embedding Space](https://aclanthology.org/2023.acl-long.893/)"},{"location":"#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Evaluation Metrics for Attribution</p> <p>We don't yet have metrics implemented for attribution methods, but that's coming soon!</p> <p>Evaluation Metrics for Concepts</p>  Several properties of the concept-space are desirable. The concept-space should (1) be faithful to the latent space data distribution; (2) have a low complexity to push toward interpretability; (3) be stable across different training regimes.    - *Concept-space faithfulness:* In Interpreto, you can use the ReconstructionError to define a custom metric by specifying a reconstruction_space and a distance_function. The MSE or FID metrics are also available. - *Concept-space complexity:* Sparsity and SparsityRatio metric are available. - *Concept-space stability:* You can use Stability metric to compare concept-model dictionaries."},{"location":"#contributing","title":"\ud83d\udc4d Contributing","text":"<p>Feel free to propose your ideas or come and contribute with us on the Interpreto \ud83e\ude84 toolbox! We have a specific document where we describe in a simple way how to make your first pull request.</p>"},{"location":"#see-also","title":"\ud83d\udc40 See Also","text":"<p>More from the DEEL project:</p> <ul> <li>Xplique a Python library dedicated to explaining neural networks (Images, Time Series, Tabular data) on TensorFlow.</li> <li>Puncc a Python library for predictive uncertainty quantification using conformal prediction.</li> <li>oodeel a Python library that performs post-hoc deep Out-of-Distribution (OOD) detection on already trained neural network image classifiers.</li> <li>deel-lip a Python library for training k-Lipschitz neural networks on TensorFlow.</li> <li>deel-torchlip a Python library for training k-Lipschitz neural networks on PyTorch.</li> <li>Influenciae a Python library dedicated to computing influence values for the discovery of potentially problematic samples in a dataset.</li> <li>DEEL White paper a summary of the DEEL team on the challenges of certifiable AI and the role of data quality, representativity and explainability for this purpose.</li> </ul>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>This project received funding from the French \u201dInvesting for the Future \u2013 PIA3\u201d program within the Artificial and Natural Intelligence Toulouse Institute (ANITI). The authors gratefully acknowledge the support of the DEEL and the FOR projects.</p>"},{"location":"#creators","title":"\ud83d\udc68\u200d\ud83c\udf93 Creators","text":"<p>Interpreto \ud83e\ude84 is a project of the FOR and the DEEL teams at the IRT Saint-Exup\u00e9ry in Toulouse, France.</p>"},{"location":"#citation","title":"\ud83d\uddde\ufe0f Citation","text":"<p>If you use Interpreto \ud83e\ude84 as part of your workflow in a scientific publication, please consider citing \ud83d\uddde\ufe0f our paper (coming soon):</p> <pre><code>BibTeX entry coming soon\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>The package is released under MIT license.</p>"},{"location":"about/","title":"About","text":"<p><code>interpreto</code> is developed as part of the Artificial and Natural Intelligence Toulouse Institute (DEEL/FOR/ANITI) program.</p> <p>DEEL/FOR/ANITI is the repository owner and the write rights manager.</p> <p>These management rules are intended to be collaborative and all those involved in the project are invited to contribute to its improvement.</p>"},{"location":"about/#functions","title":"Functions","text":""},{"location":"about/#governance-committee","title":"Governance committee","text":"<p>The governance committee is initially composed of DEEL and FOR members who contributed to the first version of <code>interpreto</code> and are the only contributors to the main branch.</p> <p>The governance committee is responsible for the main branch that contains the code of the version of the library that is officially recognized.</p> <p>These governance committee members are the only ones able to merge pull requests into this branch which come from contributions branches.</p> <p>The governance committee identifies among the contributors who by their merits can join the committee.</p>"},{"location":"about/#contributors","title":"Contributors","text":"<p>A contributor is anyone who comments on any aspects relating to the project: comments on an issue or pull request, documentation, architecture, code and validation tests or anybody with a merged pull request.</p> <p>All governance committee members are contributors.</p>"},{"location":"about/#maintainers","title":"Maintainers","text":"<p>All contributors with write rights (commit rights) are maintainers.</p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>Contributions rules are defined and developed in docs/contributing.md.</p> <p>Pull requests with major changes must be approved by at least two members of the governance committee.</p> <p>Pull requests with minor changes must be approved by at least one member of the governance committee.</p>"},{"location":"about/#moderators","title":"Moderators","text":"<ul> <li>Fanny Jourdan fanny.jourdan@irt-saintexupery.com</li> <li>Antonin Poch\u00e9 antonin.poche@irt-saintexupery.com</li> <li>Thomas Mullor thomas.mullor@irt-saintexupery.com</li> <li>Gabriele Sarti gabriele.sarti996@gmail.com</li> </ul>"},{"location":"contributing/","title":"How to contribute","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the API please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"contributing/#setup-and-dependency-installation","title":"Setup and dependency installation","text":"<ul> <li>Clone the repo <code>git clone https://github.com/FOR-sight-ai/interpreto.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd interpreto</code></li> <li>Create a virtual environment and install the necessary dependencies for development.</li> </ul> <p>We use <code>uv</code> to manage Interpreto dependencies. If you dont have <code>uv</code>, you should install with <code>make uv-download</code>.</p> <p>To install dependencies and prepare <code>pre-commit</code> hooks you would need to run:</p> <pre><code>make install # Regular dependencies for normal usage\n\nor\n\nmake install-dev # Dev dependencies including docs and linting\n</code></pre> <p>To activate your <code>.venv</code> run <code>source .venv/bin/activate</code>.</p> <p>Welcome to the team!</p>"},{"location":"contributing/#codestyle","title":"Codestyle","text":"<p>After installation you may execute code formatting.</p> <pre><code>make lint\n</code></pre> <p>Many checks are configured for this project. Command <code>make lint</code> will check style with <code>ruff</code>. We use Google style for docstrings.</p>"},{"location":"contributing/#before-submitting","title":"Before submitting","text":"<p>Before submitting your code please do the following steps:</p> <ol> <li>Add any changes you want</li> <li>Add tests for the new changes</li> <li>Edit documentation if you have changed something significant</li> <li>Run <code>make fix-style</code> to format your changes.</li> <li>Run <code>make lint</code> to ensure that formats are okay.</li> <li>Write a good commit message.</li> </ol>"},{"location":"contributing/#other-help","title":"Other help","text":"<p>You can contribute by spreading a word about this library. It would also be a huge contribution to write a short article on how you are using this project. You can also share your best practices with us.</p>"},{"location":"api/attributions/overview/","title":"API: Attributions Methods","text":""},{"location":"api/attributions/overview/#common-api","title":"Common API","text":"<pre><code>explainer = Methods(model, tokenizer, device, args)\nexplanation = explainer.explain(inputs, targets)\n</code></pre> <p>The API have two steps:</p> <ul> <li>explainer instantiation: Method is an attribution method among those displayed methods tables. It inherits from the Base class BlackBoxExplainer. Their initialization takes 2 parameters apart from the specific ones and generates an explainer:</li> <li>model: the model from which we want to obtain attributions.</li> <li>tokenizer: if the model is an NLP model, the associated tokenizer must be given, otherwise None.</li> <li>batch_size: an integer which allows to process inputs per batch.</li> <li> <p>args: specifics for each method.</p> </li> <li> <p>explain method: The call to explainer generates the explanations, it takes two parameters:</p> </li> <li>inputs: the samples on which the explanations are requested, see inputs section for more detail.</li> <li>targets: another parameter to specify what to explain in the inputs, can be a specific class or a set of classes (for classification), or texts (for generation). If targets=None, the target is calculated by making the prediction from the input with the given model.</li> </ul>"},{"location":"api/attributions/overview/#specifics-methods","title":"Specifics methods","text":"<p>TODO: add link to documentation of each method. A table would be easier to read.</p> <p>Inference-based Methods:</p> <ul> <li>Occlusion: Zeiler and Fergus, 2014. Visualizing and understanding convolutional networks.</li> <li>LIME: Ribeiro et al. 2013, \"Why should i trust you?\" explaining the predictions of any classifier.</li> <li>Kernel SHAP: Lundberg and Lee, 2017, A Unified Approach to Interpreting Model Predictions.</li> <li>Sobol Attribution: Fel et al. 2021, Look at the variance! efficient black-box explanations with sobol-based sensitivity analysis.</li> </ul> <p>Gradient based methods:</p> <ul> <li>Saliency, InputxGradient: Simonyan et al. 2013, Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.</li> <li>Integrated Gradient: Sundararajan et al. 2017, Axiomatic Attribution for Deep Networks.</li> <li>SmoothGrad: Smilkov et al. 2017, SmoothGrad: removing noise by adding noise</li> </ul>"},{"location":"api/attributions/overview/#custom-api","title":"Custom API","text":"<p>Although we've created an API that allows the user to call specific allocation methods directly, a harmonized API for all allocation methods can be used for greater freedom.</p> <p>TODO: update with the new API</p> <pre><code>inference_wrapper = InferenceWrapper(model, batch_size)\nperturbator = Perturbator(tokenizer, inputs_embedder)\naggregator = Aggregator(todo)\nexplainer = AttributionExplainer(inference_wrapper, perturbator, aggregator, device)\nexplanation = explainer.explain(inputs, targets)\n</code></pre> <p>The API have two steps:</p> <p>inference_wrapper:     - model: the model from which we want to obtain attributions.     - batch_size: an integer which allows to process inputs per batch.</p> <p>perturbator:     -</p> <ul> <li>explainer instantiation: Method is an attribution method among those displayed methods tables. It inherits from the Base class BlackBoxExplainer. Their initialization takes 2 parameters apart from the specific ones and generates an explainer:</li> <li>inference_wrapper: Callable,</li> <li>perturbator: BasePerturbator | None = None,</li> <li>aggregator: Aggregator | None = None,</li> <li> <p>device: torch.device | None = None</p> </li> <li> <p>explain method: The call to explainer generates the explanations, it takes two parameters:</p> </li> <li>inputs: the samples on which the explanations are requested, see inputs section for more detail.</li> <li>targets: another parameter to specify what to explain in the inputs, can be a specific class or a set of classes (for classification), or texts (for generation).</li> </ul>"},{"location":"api/attributions/methods/integrated_gradients/","title":"Integrated Gradients","text":"<p>\ud83d\udcf0 Paper</p> <p>Integrated Gradients (IG) is a gradient-based interpretability method that attributes importance scores to input features (e.g., tokens) by integrating the model\u2019s gradients along a path from a baseline input to the actual input. The method is designed to address some of the limitations of standard gradients, such as saturation and noise, by averaging gradients over interpolated inputs rather than relying on a single local gradient.</p> <p>Procedure:</p> <ul> <li>Choose a baseline input (e.g., a sequence of padding tokens, [MASK] tokens, or zeros) representing the absence of information.</li> <li>Linearly interpolate inputs between the baseline and the actual input over n steps.</li> <li>Compute the gradient of the model\u2019s output with respect to the input embeddings at each step.</li> <li>Approximate the integral of the gradients along the path and multiply by the input difference to get the attribution scores.</li> </ul>"},{"location":"api/attributions/methods/integrated_gradients/#example","title":"Example","text":"<pre><code>from interpreto.attributions import IntegratedGradients\n\n# load model, tokenizer and text\nmethod = IntegratedGradients(model=model, tokenizer=tokenizer, batch_size=4, n_interpolations=50)\nexplanations = method.explain(model_inputs = text)\n</code></pre>"},{"location":"api/attributions/methods/integrated_gradients/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain.</li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model.</li> <li>batch_size (int): batch size for the attribution method.</li> <li>device (torch.device): device on which the attribution method will be run.</li> <li>n_interpolations (int): Number of points to interpolate between the baseline and the desired point.</li> <li>baseline (torch.Tensor | float | None): the baseline to use for the interpolations.</li> </ul>"},{"location":"api/attributions/methods/kernelshap/","title":"KernelSHAP","text":"<p>\ud83d\udcf0 Paper</p> <p>KernelSHAP is a model\u2011agnostic Shapley value estimator that interprets predictions by computing Shapley values through a weighted linear regression in the space of feature coalitions. By unifying ideas from LIME and Shapley value theory, KernelSHAP provides additive feature attributions with strong consistency guarantees.</p>"},{"location":"api/attributions/methods/kernelshap/#example","title":"Example","text":"<pre><code>from interpreto import Granularity, KernelShap\nfrom interpreto.attributions import InferenceModes\n\n# load model, tokenizer and text\nmethod = KernelShap(model, tokenizer, batch_size=4, \n    inference_mode=InferenceModes.SOFTMAX,\n    n_perturbations=20,\n    granularity=Granularity.WORD)\nexplanations = method(text)\n</code></pre>"},{"location":"api/attributions/methods/kernelshap/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain  </li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model  </li> <li>batch_size (int): batch size for the attribution method  </li> <li>inference_mode (InferenceModes): output space to analyse (e.g., InferenceModes.LOGITS, InferenceModes.SOFTMAX, InferenceModes.LOG_SOFTMAX)  </li> <li>n_perturbations (int): number of perturbations (coalitions) used to estimate Shapley values  </li> <li>granularity (Granularity): granularity level of the perturbations (token, word, sentence, etc.)  </li> <li>device (torch.device): device on which the attribution method will be run  </li> </ul>"},{"location":"api/attributions/methods/lime/","title":"LIME","text":"<p>\ud83d\udcf0 Paper</p> <p>Local Interpretable Model\u2011agnostic Explanations (LIME) is a perturbation\u2011based approach that explains individual predictions by fitting a simple, interpretable surrogate model locally around the prediction of interest. By sampling perturbed versions of the input and weighting them by their proximity to the original instance, LIME learns per\u2011feature importance scores that approximate the behaviour of the underlying black\u2011box model in that local region.</p>"},{"location":"api/attributions/methods/lime/#example","title":"Example","text":"<pre><code>from interpreto import Granularity, Lime\nfrom interpreto.attributions import InferenceModes\n\n# load model, tokenizer and text\nmethod = Lime(model, tokenizer, batch_size=4, \n    inference_mode=InferenceModes.LOG_SOFTMAX,\n    n_perturbations=20,\n    granularity=Granularity.WORD,\n    distance_function=Lime.distance_functions.HAMMING)\nexplanations = method(text)\n</code></pre>"},{"location":"api/attributions/methods/lime/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain  </li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model  </li> <li>batch_size (int): batch size for the attribution method  </li> <li>inference_mode (InferenceModes): output space to analyse (e.g., InferenceModes.LOGITS, InferenceModes.SOFTMAX, InferenceModes.LOG_SOFTMAX)  </li> <li>n_perturbations (int): number of perturbed samples used to train the local surrogate model  </li> <li>granularity (Granularity): granularity level of the perturbations (token, word, sentence, etc.)  </li> <li>distance_function (Callable): function to compute similarity between original and perturbed samples (e.g., Lime.distance_functions.HAMMING)  </li> <li>device (torch.device): device on which the attribution method will be run  </li> </ul>"},{"location":"api/attributions/methods/occlusion/","title":"Occlusion","text":"<p>\ud83d\udcf0 Paper</p> <p>The Occlusion method is a perturbation-based approach to interpret model behavior by analyzing the impact of removing or masking parts of the input text. The principle is simple: by systematically occluding (i.e., masking, deleting, or replacing) specific tokens or spans in the input and observing how the model's output changes, one can infer the relative importance of each part of the input to the model's behavior.</p>"},{"location":"api/attributions/methods/occlusion/#example","title":"Example","text":"<pre><code>from interpreto import Granularity, Occlusion\nfrom interpreto.attributions import InferenceModes\n\n# load model, tokenizer and text\nmethod = Occlusion(model, tokenizer, batch_size=4,\n    inference_mode=InferenceModes.SOFTMAX,\n    granularity=Granularity.WORD)\nexplanations = method(text)\n</code></pre>"},{"location":"api/attributions/methods/occlusion/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain</li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model</li> <li>batch_size (int): batch size for the attribution method</li> <li>granularity (Granularity): granularity level of the perturbations (token, word, sentence, etc.)</li> <li>device (torch.device): device on which the attribution method will be run</li> <li>replace_token_id (int): token id to use for replacing the occluded tokens</li> </ul>"},{"location":"api/attributions/methods/saliency/","title":"Saliency","text":"<p>\ud83d\udcf0 Paper</p> <p>Saliency maps are a simple and widely used gradient-based method for interpreting neural network predictions. The idea is to compute the gradient of the model's output with respect to its input embeddings to estimate which input tokens most influence the output.</p> <p>Procedure:</p> <ul> <li>Pass the input through the model to obtain an output (e.g., class logit, token probability).</li> <li>Compute the gradient of the output with respect to the input embeddings.</li> <li>For each token, reduce the gradient vector (e.g., via norm with the embedding) to obtain a scalar importance score.</li> </ul>"},{"location":"api/attributions/methods/saliency/#example","title":"Example","text":"<pre><code>from interpreto.attributions import Saliency\n\n# load model, tokenizer and text\nmethod = Saliency(model, tokenizer, batch_size=4)\nexplanations = method.explain(text)\n</code></pre>"},{"location":"api/attributions/methods/saliency/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain</li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model</li> <li>batch_size (int): batch size for the attribution method</li> <li>device (torch.device): device on which the attribution method will be run</li> </ul>"},{"location":"api/attributions/methods/smoothgrad/","title":"Smoothgrad","text":"<p>\ud83d\udcf0 Paper</p> <p>SmoothGrad is an enhanced version of gradient-based interpretability methods, such as saliency maps. It reduces the noise and visual instability often seen in raw gradient attributions by averaging gradients over multiple noisy versions of the input. The result is a smoothed importance score for each token.</p> <p>Procedure:</p> <ul> <li>Generate multiple perturbed versions of the input by adding noise (Gaussian) to the input embeddings.</li> <li>For each noisy input, compute the gradient of the output with respect to the embeddings.</li> <li>Average the gradients across all samples.</li> <li>Aggregate the result per token (e.g., by norm with the input) to get the final attribution scores.</li> </ul>"},{"location":"api/attributions/methods/smoothgrad/#example","title":"Example","text":"<pre><code>from interpreto.attributions import Smoothgrad\n\n# load model, tokenizer and text\nmethod = Smoothgrad(model, tokenizer, batch_size=4, n_interpolations=50, noise_level=0.01)\nexplanations = method.explain(text)\n</code></pre>"},{"location":"api/attributions/methods/smoothgrad/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain.</li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model.</li> <li>batch_size (int): batch size for the attribution method.</li> <li>device (torch.device): device on which the attribution method will be run.</li> <li>n_interpolations (int): the number of interpolations to generate multiple perturbed versions of the input by adding noise.</li> <li>noise_level (float): standard deviation of the Gaussian noise to add to the inputs.</li> </ul>"},{"location":"api/attributions/methods/sobol/","title":"Sobol Attribution","text":"<p>\ud83d\udcf0 Paper</p> <p>Sobol is a variance-based sensitivity analysis method used to quantify the contribution of each input component to the output variance of the model. It estimates both the first-order (main) and total (interaction) effects of features using Monte Carlo sampling strategies. In NLP, Sobol helps assess which words or tokens are most influential for the model\u2019s decision, including how they interact with one another.</p>"},{"location":"api/attributions/methods/sobol/#example","title":"Example","text":"<pre><code>from interpreto import Granularity, Sobol\nfrom interpreto.attributions import InferenceModes\n\n# load model, tokenizer and text\nmethod = Sobol(model, tokenizer, batch_size=4, \n    inference_mode=InferenceModes.LOGITS,\n    n_token_perturbations=8,\n    granularity=Granularity.WORD)\nexplanations = method(text)\n</code></pre>"},{"location":"api/attributions/methods/sobol/#parameters","title":"Parameters","text":"<p>TODO: use mkdocs built in functionalities to generate the table and put example in the docstring</p> <ul> <li>model (PreTrainedModel): Hugging Face model to explain  </li> <li>tokenizer (PreTrainedTokenizer): Hugging Face tokenizer associated with the model  </li> <li>batch_size (int): batch size for the attribution method  </li> <li>granularity (Granularity): granularity level of the perturbations (token, word, sentence, etc.)  </li> <li>inference_mode (InferenceModes): output space to analyse (e.g., InferenceModes.LOGITS, InferenceModes.SOFTMAX, InferenceModes.LOG_SOFTMAX)  </li> <li>n_token_perturbations (int): number of perturbation samples used to estimate Sobol indices  </li> <li>sobol_indices_order (SobolIndicesOrders): order of the Sobol indices to estimate, either <code>FIRST_ORDER</code> or <code>TOTAL_ORDER</code> </li> <li>sampler (SequenceSamplers): sampling method used to generate perturbations, one of <code>SOBOL</code>, <code>HALTON</code>, or <code>LATIN_HYPERCUBE</code> </li> <li>device (torch.device): device on which the attribution method will be run  </li> </ul>"},{"location":"api/commons/model_wrapping/","title":"Model Wrapping","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints","title":"interpreto.model_wrapping.ModelWithSplitPoints","text":"<pre><code>ModelWithSplitPoints(model_or_repo_id, split_points, *args, model_autoclass=None, tokenizer=None, config=None, **kwargs)\n</code></pre> <p>               Bases: <code>LanguageModel</code></p> <p>Code:  model_wrapping/model_with_split_points.py` </p> <p>Generalized NNsight.LanguageModel wrapper around encoder-only, decoder-only and encoder-decoder language models. Handles splitting model at specified locations and activation extraction.</p> <p>Inputs can be in the form of:</p> <pre><code>* One (`str`) or more (`list[str]`) prompts, including batched prompts (`list[list[str]]`).\n\n* One (`list[int] or torch.Tensor`) or more (`list[list[int]] or torch.Tensor`) tokenized prompts.\n\n* Direct model inputs: (`dic[str,Any]`)\n</code></pre> <p>Attributes:</p> Name Type Description <code>model_autoclass</code> <code>type</code> <p>The AutoClass corresponding to the loaded model type.</p> <code>split_points</code> <code>list[str]</code> <p>Getter/setters for model paths corresponding to split points inside the loaded model. Automatically handle validation, sorting and resolving int paths to strings.</p> <code>repo_id</code> <code>str</code> <p>Either the model id in the HF Hub, or the path from which the model was loaded.</p> <code>generator</code> <code>Envoy | None</code> <p>If the model is generative, a generator is provided to handle multi-step inference. None for encoder-only models.</p> <code>_model</code> <code>PreTrainedModel</code> <p>Huggingface transformers model wrapped by NNSight.</p> <code>_model_paths</code> <code>list[str]</code> <p>List of cached valid paths inside <code>_model</code>, used to validate <code>split_points</code>.</p> <code>_split_points</code> <code>list[str]</code> <p>List of split points, should be accessed with getter/setter.</p> <p>Parameters:</p> Name Type Description Default <code>str | PreTrainedModel</code> <p>One of:</p> <ul> <li>A <code>str</code> corresponding to the ID of the model that should be loaded from the HF Hub.</li> <li>A <code>str</code> corresponding to the local path of a folder containing a compatible checkpoint.</li> <li>A preloaded <code>transformers.PreTrainedModel</code> object. If a string is provided, a model_autoclass should also be provided.</li> </ul> required <code>str | Sequence[str] | int | Sequence[int]</code> <p>One or more to split locations inside the model. Either the path is provided explicitly (<code>str</code>), or an <code>int</code> is used as shorthand for splitting at the n-th layer. Example: <code>split_points='cls.predictions.transform.LayerNorm'</code> correspond to a split after the LayerNorm layer in the MLM head (assuming a <code>BertForMaskedLM</code> model in input).</p> required <code>Type</code> <p>Huggingface AutoClass corresponding to the desired type of model (e.g. <code>AutoModelForSequenceClassification</code>).</p> <p> <code>model_autoclass</code> must be defined if <code>model_or_repo_id</code> is <code>str</code>, since the the model class     cannot be known otherwise.</p> <code>None</code> <code>PretrainedConfig</code> <p>Custom configuration for the loaded model. If not specified, it will be instantiated with the default configuration for the model.</p> <code>None</code> <code>PreTrainedTokenizer</code> <p>Custom tokenizer for the loaded model. If not specified, it will be instantiated with the default tokenizer for the model.</p> <code>None</code>"},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints(model_or_repo_id)","title":"<code>model_or_repo_id</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints(split_points)","title":"<code>split_points</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints(model_autoclass)","title":"<code>model_autoclass</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints(config)","title":"<code>config</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints(tokenizer)","title":"<code>tokenizer</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints.get_activations","title":"get_activations","text":"<pre><code>get_activations(inputs, select_strategy=FLATTEN, select_indices=None, **kwargs)\n</code></pre> <p>Get intermediate activations for all model split points</p> <p>Parameters:</p> Name Type Description Default <code>str | list[str] | BatchEncoding | Tensor</code> <p>Inputs to the model forward pass before or after tokenization.</p> required <code>str | ActivationSelectionStrategy</code> <p>Selection strategy for activations.</p> <p>Options are:</p> <ul> <li><code>all</code>: All sequence activations are returned, keeping the original shape <code>(batch, seq_len, d_model)</code>.</li> <li><code>flatten</code>: Every token activation is treated as a separate element - <code>(batch x seq_len, d_model)</code>.</li> </ul> <code>FLATTEN</code> <code>list[int] | tuple[int] | None</code> <p>Specifies indices that should be selected from the activation sequence. Can be combined with <code>select_strategy</code> to obtain several behaviors. E.g. <code>select_strategy=\"all\", select_indices=mask_idxs</code> can be used to extract only activations corresponding to [MASK] input ids with shape <code>(batch, len(mask_idxs), d_model)</code>, or <code>select_strategy=\"flatten\", select_indices=0</code> can be used to extract activations for the <code>[CLS]</code> token only across all sequences, with shape <code>(batch, d_model)</code>. By default, all positions are selected.</p> <code>None</code> <p>Returns:</p> Type Description <code>InterventionProxy</code> <p>(InterventionProxy) Dictionary having one key, value pair for each split point defined for the model. Keys correspond to split names in <code>self.split_points</code>, while values correspond to the extracted activations for the split point for the given <code>inputs</code>.</p>"},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints.get_activations(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints.get_activations(select_strategy)","title":"<code>select_strategy</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints.get_activations(select_indices)","title":"<code>select_indices</code>","text":""},{"location":"api/commons/model_wrapping/#interpreto.model_wrapping.ModelWithSplitPoints.get_latent_shape","title":"get_latent_shape","text":"<pre><code>get_latent_shape(inputs=None)\n</code></pre> <p>Get the shape of the latent activations at the specified split point.</p>"},{"location":"api/concepts/overview/","title":"API General","text":"<pre><code>model_with_split_points = ModelWithSplitPoints(model, tokenizer, split_points)\nconcept_explainer = TopKSAEExplainer(model_with_split_points)\n\nactivations = model_with_split_points.get_activations(dataset, split_point)\nconcept_explainer.fit(activations)\n\n\nintegrated_gradient_kwargs = {\"step\": 10, \"baseline\": \"[MASK]\"}\nconcept_input_attribution = concept_explainer.input_concept_attribution(attribution_method=IntegratedGradient, examples, **integrated_gradient_kwargs)\n\nTopKtokens_kwargs = {todo}\nconcept_interpretation = concept_explainer.interpret(interpretation_method=TopKtokens, examples, **TopKtokens_kwargs)\n</code></pre>"},{"location":"api/concepts/overview/#custom-api","title":"Custom API","text":"<pre><code>model_with_split_points = ModelWithSplitPoints(model, tokenizer, split_points)\nconcept_model = SAEExtraction(concept_model=OvercompleteSAEClasses.TopKSAE)\nactivations = model_with_split_points.get_activations(dataset, split_point)\n\nconcept_model.fit(activations)\n\nconcept_explainer = ConceptAutoEncoderExplainer(model_with_split_points, concept_model)\n\nintegrated_gradient_kwargs = {\"step\": 10, \"baseline\": \"[MASK]\"}\nconcept_input_attribution = concept_explainer.input_concept_attribution(attribution_method=IntegratedGradient, examples, **integrated_gradient_kwargs)\n\nTopKtokens_kwargs = {todo}\nconcept_interpretation = concept_explainer.interpret(interpretation_method=TopKtokens, examples, **TopKtokens_kwargs)\n</code></pre>"},{"location":"api/concepts/interpretations/base/","title":"Base Classes","text":""},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod","title":"interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod","text":"<pre><code>BaseConceptInterpretationMethod(model_with_split_points, concept_model, split_point=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Code:  <code>concepts/interpretations/base.py</code> </p> <p>Abstract class defining an interface for concept interpretation. Its goal is to make the dimensions of the concept space interpretable by humans.</p> <p>Attributes:</p> Source code in <code>interpreto/concepts/interpretations/base.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    concept_model: ConceptModelProtocol,\n    split_point: str | None = None,\n):\n    if not hasattr(concept_model, \"encode\"):\n        raise TypeError(\n            f\"Concept model should be able to encode activations into concepts. Got: {type(concept_model)}.\"\n        )\n\n    if split_point is None:\n        if len(model_with_split_points.split_points) &gt; 1:\n            raise ValueError(\n                \"If the model has more than one split point, a split point for fitting the concept model should \"\n                f\"be specified. Got split point: '{split_point}' with model split points: \"\n                f\"{', '.join(model_with_split_points.split_points)}.\"\n            )\n        split_point = model_with_split_points.split_points[0]\n\n    if split_point not in model_with_split_points.split_points:\n        raise ValueError(\n            f\"Split point '{split_point}' not found in model split points: \"\n            f\"{', '.join(model_with_split_points.split_points)}.\"\n        )\n\n    self.model_with_split_points: ModelWithSplitPoints = model_with_split_points\n    self.split_point: str = split_point\n    self.concept_model: ConceptModelProtocol = concept_model\n</code></pre>"},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod.interpret","title":"interpret  <code>abstractmethod</code>","text":"<pre><code>interpret(concepts_indices, inputs=None, latent_activations=None, concepts_activations=None)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>int | list[int]</code> <p>The indices of the concepts to interpret.</p> required <code>ModelInputs | None</code> <p>The inputs to use for the interpretation. Necessary if the source is not <code>VOCABULARY</code>, as examples are extracted from the inputs.</p> <code>None</code> <code>LatentActivations | None</code> <p>The latent activations to use for the interpretation. Necessary if the source is <code>LATENT_ACTIVATIONS</code>. Otherwise, it is computed from the inputs or ignored if the source is <code>CONCEPT_ACTIVATIONS</code>.</p> <code>None</code> <code>ConceptsActivations | None</code> <p>The concepts activations to use for the interpretation. Necessary if the source is not <code>CONCEPT_ACTIVATIONS</code>. Otherwise, it is computed from the latent activations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: The interpretation of each of the specified concepts.</p> Source code in <code>interpreto/concepts/interpretations/base.py</code> <pre><code>@abstractmethod\ndef interpret(\n    self,\n    concepts_indices: int | list[int],\n    inputs: ModelInputs | None = None,\n    latent_activations: LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        concepts_indices (int | list[int]): The indices of the concepts to interpret.\n        inputs (ModelInputs | None): The inputs to use for the interpretation.\n            Necessary if the source is not `VOCABULARY`, as examples are extracted from the inputs.\n        latent_activations (LatentActivations | None): The latent activations to use for the interpretation.\n            Necessary if the source is `LATENT_ACTIVATIONS`.\n            Otherwise, it is computed from the inputs or ignored if the source is `CONCEPT_ACTIVATIONS`.\n        concepts_activations (ConceptsActivations | None): The concepts activations to use for the interpretation.\n            Necessary if the source is not `CONCEPT_ACTIVATIONS`. Otherwise, it is computed from the latent activations.\n\n    Returns:\n        Mapping[int, Any]: The interpretation of each of the specified concepts.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod.interpret(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod.interpret(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/interpretations/base/#interpreto.concepts.interpretations.base.BaseConceptInterpretationMethod.interpret(concepts_activations)","title":"<code>concepts_activations</code>","text":""},{"location":"api/concepts/interpretations/overview/","title":"Concept Interpretations","text":"<p>TODO: Add a brief description of the concept interpretations.</p>"},{"location":"api/concepts/interpretations/topk_inputs/","title":"TopKInputs or MaxAct","text":"<p>Generalization of maximally activating inputs used by Towards Monosemanticity: Decomposing Language Models With Dictionary Learning by Bricken et al. (2023)</p>"},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.Granularities","title":"interpreto.concepts.interpretations.Granularities","text":"<p>               Bases: <code>Enum</code></p> <p>Code  <code>concepts/interpretations/topk_inputs.py</code></p> <p>Possible granularities of inputs returned by the Top-K Inputs concept interpretation method.</p> <p>Valid granularities are:</p> <ul> <li><code>TOKENS</code>: the granularity is at the token level.</li> </ul>"},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.InterpretationSources","title":"interpreto.concepts.interpretations.InterpretationSources","text":"<p>               Bases: <code>Enum</code></p> <p>Code  <code>concepts/interpretations/topk_inputs.py</code></p> <p>Possible sources of inputs to use for the Top-K Inputs concept interpretation method. The activations do not need to take into account the granularity of the inputs. It is managed internally.</p> <p>Valid sources are:</p> <ul> <li> <p><code>CONCEPTS_ACTIVATIONS</code>: also require <code>inputs</code> to return strings but assume that the <code>concepts_activations</code> are provided and correspond to the inputs. Hence it is the fastest source.</p> </li> <li> <p><code>LATENT_ACTIVATIONS</code>: also require <code>inputs</code> to return strings but assume that the <code>latent_activations</code> are provided and correspond to the inputs.     The latent activations can be the one used to fit the <code>concepts_model</code>. Hence the easiest source to use.</p> </li> <li> <p><code>INPUTS</code>: requires <code>inputs</code> and compute activations on them to extract the most activating inputs. It is the slowest source.</p> </li> <li> <p><code>VOCABULARY</code>: each token of the tokenizer vocabulary is considered as an <code>inputs</code>, then activations are computed. This source has the least requirements.</p> </li> <li> <p><code>AUTO</code>: depending on the provided arguments, it will select the most appropriate source. Order of preference is:</p> <ol> <li><code>CONCEPTS_ACTIVATIONS</code></li> <li><code>LATENT_ACTIVATIONS</code></li> <li><code>INPUTS</code></li> <li><code>VOCABULARY</code></li> </ol> </li> </ul>"},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs","title":"interpreto.concepts.interpretations.TopKInputs","text":"<pre><code>TopKInputs(*, model_with_split_points, concept_model, granularity, source, split_point=None, k=5)\n</code></pre> <p>               Bases: <code>BaseConceptInterpretationMethod</code></p> <p>Code  <code>concepts/interpretations/topk_inputs.py</code></p> <p>Implementation of the Top-K Inputs concept interpretation method also called MaxAct. It is the most natural way to interpret a concept, as it is the most natural way to explain a concept. Hence several papers used it without describing it. Nonetheless, we can reference Bricken et al. (2023) <sup>1</sup> from Anthropic for their post on transformer-circuits.</p> <ol> <li> <p>Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn*, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Transformer Circuits, 2023.\u00a0\u21a9</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model with split points to use for the interpretation.</p> <code>split_point</code> <code>str</code> <p>The split point to use for the interpretation.</p> <code>concept_model</code> <code>ConceptModelProtocol</code> <p>The concept model to use for the interpretation.</p> <code>granularity</code> <code>Granularities</code> <p>The granularity at which the interpretation is computed. Ignored for source <code>VOCABULARY</code>.</p> <code>source</code> <code>InterpretationSources</code> <p>The source of the inputs to use for the interpretation.</p> <code>k</code> <code>int</code> <p>The number of inputs to use for the interpretation.</p> Source code in <code>interpreto/concepts/interpretations/topk_inputs.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_with_split_points: ModelWithSplitPoints,\n    concept_model: ConceptModelProtocol,\n    granularity: Granularities,\n    source: InterpretationSources,\n    split_point: str | None = None,\n    k: int = 5,\n):\n    super().__init__(\n        model_with_split_points=model_with_split_points, concept_model=concept_model, split_point=split_point\n    )\n\n    if granularity is not Granularities.TOKENS:\n        raise NotImplementedError(\"Only token granularity is currently supported for interpretation.\")\n\n    if source not in InterpretationSources:\n        raise ValueError(f\"The source {source} is not supported. Supported sources: {InterpretationSources}\")\n\n    self.granularity = granularity\n    self.source = source\n    self.k = k\n</code></pre>"},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs.interpret","title":"interpret","text":"<pre><code>interpret(concepts_indices, inputs=None, latent_activations=None, concepts_activations=None)\n</code></pre> <p>Give the interpretation of the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and a list of inputs allowing to interpret them. The granularity of input examples is determined by the <code>granularity</code> class attribute.</p> <p>The returned inputs are the most activating inputs for the concepts.</p> <p>The required arguments depend on the <code>source</code> class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>int | list[int]</code> <p>The indices of the concepts to interpret.</p> required <code>list[str] | None</code> <p>The inputs to use for the interpretation. Necessary if the source is not <code>VOCABULARY</code>, as examples are extracted from the inputs.</p> <code>None</code> <code>Float[Tensor, 'nl d'] | None</code> <p>The latent activations to use for the interpretation. Necessary if the source is <code>LATENT_ACTIVATIONS</code>. Otherwise, it is computed from the inputs or ignored if the source is <code>CONCEPT_ACTIVATIONS</code>.</p> <code>None</code> <code>Float[Tensor, 'nl cpt'] | None</code> <p>The concepts activations to use for the interpretation. Necessary if the source is not <code>CONCEPT_ACTIVATIONS</code>. Otherwise, it is computed from the latent activations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: The interpretation of the concepts indices.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the arguments do not correspond to the specified source.</p> Source code in <code>interpreto/concepts/interpretations/topk_inputs.py</code> <pre><code>def interpret(\n    self,\n    concepts_indices: int | list[int],\n    inputs: list[str] | None = None,\n    latent_activations: LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Give the interpretation of the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and a list of inputs allowing to interpret them.\n    The granularity of input examples is determined by the `granularity` class attribute.\n\n    The returned inputs are the most activating inputs for the concepts.\n\n    The required arguments depend on the `source` class attribute.\n\n    Args:\n        concepts_indices (int | list[int]): The indices of the concepts to interpret.\n        inputs (list[str] | None): The inputs to use for the interpretation.\n            Necessary if the source is not `VOCABULARY`, as examples are extracted from the inputs.\n        latent_activations (Float[torch.Tensor, \"nl d\"] | None): The latent activations to use for the interpretation.\n            Necessary if the source is `LATENT_ACTIVATIONS`.\n            Otherwise, it is computed from the inputs or ignored if the source is `CONCEPT_ACTIVATIONS`.\n        concepts_activations (Float[torch.Tensor, \"nl cpt\"] | None): The concepts activations to use for the interpretation.\n            Necessary if the source is not `CONCEPT_ACTIVATIONS`. Otherwise, it is computed from the latent activations.\n\n    Returns:\n        Mapping[int, Any]: The interpretation of the concepts indices.\n\n    Raises:\n        ValueError: If the arguments do not correspond to the specified source.\n    \"\"\"\n    # compute the concepts activations from the provided source, can also create inputs from the vocabulary\n    sure_inputs: list[str]  # Verified by concepts_activations_from_source\n    sure_concepts_activations: Float[torch.Tensor, \"nl cpt\"]  # Verified by concepts_activations_from_source\n    sure_inputs, sure_concepts_activations = self._concepts_activations_from_source(\n        inputs, latent_activations, concepts_activations\n    )\n\n    granular_inputs: list[str]  # len: ng, inputs becomes a list of elements extracted from the examples\n    granular_concepts_activations: Float[torch.Tensor, \"ng cpt\"]\n    granular_inputs, granular_concepts_activations = self._get_granular_inputs(\n        inputs=sure_inputs, concepts_activations=sure_concepts_activations\n    )\n\n    concepts_indices = self._verify_concepts_indices(\n        concepts_activations=granular_concepts_activations, concepts_indices=concepts_indices\n    )\n\n    return self._topk_inputs_from_concepts_activations(\n        inputs=granular_inputs,\n        concepts_activations=granular_concepts_activations,\n        concepts_indices=concepts_indices,\n    )\n</code></pre>"},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs.interpret(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs.interpret(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/interpretations/topk_inputs/#interpreto.concepts.interpretations.TopKInputs.interpret(concepts_activations)","title":"<code>concepts_activations</code>","text":""},{"location":"api/concepts/methods/base/","title":"Base Classes","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer","title":"interpreto.concepts.ConceptEncoderExplainer","text":"<pre><code>ConceptEncoderExplainer(model_with_split_points, concept_model, split_point=None)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[ConceptModel]</code></p> <p>Code:  <code>concepts/base.py</code> </p> <p>Abstract class defining an interface for concept explanation. Child classes should implement the <code>fit</code> and <code>encode_activations</code> methods, and only assume the presence of an     encoding step using the <code>concept_model</code> to convert activations to latent concepts.</p> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str</code> <p>The split point used to train the <code>concept_model</code>.</p> <code>concept_model</code> <code>ConceptModelProtocol</code> <p>The model used to extract concepts from the activations of <code>model_with_split_points</code>. The only assumption for classes inheriting from this class is that the <code>concept_model</code> can encode activations into concepts with <code>encode_activations</code>. The <code>ConceptModelProtocol</code> is defined in <code>interpreto.typing</code>. It is basically a <code>torch.nn.Module</code> with an <code>encode</code> method.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>ConceptModelProtocol</code> <p>The model used to extract concepts from the activations of <code>model_with_split_points</code>. The <code>ConceptModelProtocol</code> is defined in <code>interpreto.typing</code>. It is basically a <code>torch.nn.Module</code> with an <code>encode</code> method.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> Source code in <code>interpreto/concepts/base.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    concept_model: ConceptModelProtocol,\n    split_point: str | None = None,\n):\n    \"\"\"Initializes the concept explainer with a given splitted model.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        concept_model (ConceptModelProtocol): The model used to extract concepts from\n            the activations of `model_with_split_points`.\n            The `ConceptModelProtocol` is defined in `interpreto.typing`. It is basically a `torch.nn.Module` with an `encode` method.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n    \"\"\"\n    if not isinstance(model_with_split_points, ModelWithSplitPoints):\n        raise TypeError(\n            f\"The given model should be a ModelWithSplitPoints, but {type(model_with_split_points)} was given.\"\n        )\n    self.model_with_split_points: ModelWithSplitPoints = model_with_split_points\n    self._concept_model = concept_model\n    self.split_point = split_point  # Verified by `split_point.setter`\n    self.__is_fitted: bool = False\n    self.has_differentiable_concept_encoder = False\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer(concept_model)","title":"<code>concept_model</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(activations, *args, **kwargs)\n</code></pre> <p>Fits <code>concept_model</code> on the given activations.</p> <p>Parameters:</p> Name Type Description Default <code>dict[str, Tensor]</code> <p>A dictionary with model paths as keys and the corresponding tensors as values.</p> required <p>Returns:</p> Type Description <code>Any</code> <p><code>None</code>, <code>concept_model</code> is fitted in-place, <code>is_fitted</code> is set to <code>True</code> and <code>split_point</code> is set.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@abstractmethod\ndef fit(self, activations: LatentActivations | InterventionProxy, *args, **kwargs) -&gt; Any:\n    \"\"\"Fits `concept_model` on the given activations.\n\n    Args:\n        activations (dict[str, torch.Tensor]): A dictionary with model paths as keys and the corresponding\n            tensors as values.\n\n    Returns:\n        `None`, `concept_model` is fitted in-place, `is_fitted` is set to `True` and `split_point` is set.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.fit(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, attribution_method, **attribution_kwargs)\n</code></pre> <p>Attributes model inputs for a selected concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences or a dataset.</p> required <code>int</code> <p>Index identifying the position of the concept of interest (score in the <code>ConceptsActivations</code> tensor) for which relevant input elements should be retrieved.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef input_concept_attribution(\n    self,\n    inputs: ModelInputs,\n    concept: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Attributes model inputs for a selected concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences\n            or a dataset.\n        concept (int): Index identifying the position of the concept of interest (score in the\n            `ConceptsActivations` tensor) for which relevant input elements should be retrieved.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    raise NotImplementedError(\"Input-to-concept attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptEncoderExplainer.input_concept_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer","title":"interpreto.concepts.ConceptAutoEncoderExplainer","text":"<pre><code>ConceptAutoEncoderExplainer(model_with_split_points, concept_model, split_point=None)\n</code></pre> <p>               Bases: <code>ConceptEncoderExplainer[BaseDictionaryLearning]</code>, <code>Generic[BDL]</code></p> <p>Code:  <code>concepts/base.py</code> </p> <p>A concept bottleneck explainer wraps a <code>concept_model</code> that should be able to encode activations into concepts and decode concepts into activations.</p> <p>We use the term \"concept bottleneck\" loosely, as the latent space can be overcomplete compared to activation     space, as in the case of sparse autoencoders.</p> <p>We assume that the concept model follows the structure of an <code>overcomplete.BaseDictionaryLearning</code> model, which defines the <code>encode</code> and <code>decode</code> methods for encoding and decoding activations into concepts.</p> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str</code> <p>The split point used to train the <code>concept_model</code>.</p> <code>concept_model</code> <code>[BaseDictionaryLearning](https</code> <p>//github.com/KempnerInstitute/overcomplete/blob/24568ba5736cbefca4b78a12246d92a1be04a1f4/overcomplete/base.py#L10)): The model used to extract concepts from the activations of  <code>model_with_split_points</code>. The only assumption for classes inheriting from this class is that the <code>concept_model</code> can encode activations into concepts with <code>encode_activations</code>.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <code>has_differentiable_concept_decoder</code> <code>bool</code> <p>Whether the <code>decode_concepts</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>[BaseDictionaryLearning](https</code> <p>//github.com/KempnerInstitute/overcomplete/blob/24568ba5736cbefca4b78a12246d92a1be04a1f4/overcomplete/base.py#L10)): The model used to extract concepts from the activations of <code>model_with_split_points</code>.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> Source code in <code>interpreto/concepts/base.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    concept_model: BaseDictionaryLearning,\n    split_point: str | None = None,\n):\n    \"\"\"Initializes the concept explainer with a given splitted model.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        concept_model ([BaseDictionaryLearning](https://github.com/KempnerInstitute/overcomplete/blob/24568ba5736cbefca4b78a12246d92a1be04a1f4/overcomplete/base.py#L10)): The model used to extract concepts from\n            the activations of `model_with_split_points`.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n    \"\"\"\n    self.concept_model: BaseDictionaryLearning\n    super().__init__(model_with_split_points, concept_model, split_point)\n    self.has_differentiable_concept_decoder = False\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer(concept_model)","title":"<code>concept_model</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.fit","title":"fit  <code>abstractmethod</code>","text":"<pre><code>fit(activations, *args, **kwargs)\n</code></pre> <p>Fits <code>concept_model</code> on the given activations.</p> <p>Parameters:</p> Name Type Description Default <code>dict[str, Tensor]</code> <p>A dictionary with model paths as keys and the corresponding tensors as values.</p> required <p>Returns:</p> Type Description <code>Any</code> <p><code>None</code>, <code>concept_model</code> is fitted in-place, <code>is_fitted</code> is set to <code>True</code> and <code>split_point</code> is set.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@abstractmethod\ndef fit(self, activations: LatentActivations | InterventionProxy, *args, **kwargs) -&gt; Any:\n    \"\"\"Fits `concept_model` on the given activations.\n\n    Args:\n        activations (dict[str, torch.Tensor]): A dictionary with model paths as keys and the corresponding\n            tensors as values.\n\n    Returns:\n        `None`, `concept_model` is fitted in-place, `is_fitted` is set to `True` and `split_point` is set.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.fit(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.encode_activations","title":"encode_activations","text":"<pre><code>encode_activations(activations)\n</code></pre> <p>Encode the given activations using the <code>concept_model</code> encoder.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations</code> <p>The activations to encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded concept activations.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef encode_activations(self, activations: LatentActivations) -&gt; torch.Tensor:  # ConceptsActivations\n    \"\"\"Encode the given activations using the `concept_model` encoder.\n\n    Args:\n        activations (LatentActivations): The activations to encode.\n\n    Returns:\n        The encoded concept activations.\n    \"\"\"\n    self._sanitize_activations(activations)\n    return self.concept_model.encode(activations)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.encode_activations(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.decode_concepts","title":"decode_concepts","text":"<pre><code>decode_concepts(concepts)\n</code></pre> <p>Decode the given concepts using the <code>concept_model</code> decoder.</p> <p>Parameters:</p> Name Type Description Default <code>ConceptsActivations</code> <p>The concepts to decode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The decoded model activations.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef decode_concepts(self, concepts: ConceptsActivations) -&gt; torch.Tensor:  # LatentActivations\n    \"\"\"Decode the given concepts using the `concept_model` decoder.\n\n    Args:\n        concepts (ConceptsActivations): The concepts to decode.\n\n    Returns:\n        The decoded model activations.\n    \"\"\"\n    return self.concept_model.decode(concepts)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.decode_concepts(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.get_dictionary","title":"get_dictionary","text":"<pre><code>get_dictionary()\n</code></pre> <p>Get the dictionary learned by the fitted <code>concept_model</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A <code>torch.Tensor</code> containing the learned dictionary.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef get_dictionary(self) -&gt; torch.Tensor:  # TODO: add this to tests\n    \"\"\"Get the dictionary learned by the fitted `concept_model`.\n\n    Returns:\n        torch.Tensor: A `torch.Tensor` containing the learned dictionary.\n    \"\"\"\n    return self.concept_model.get_dictionary()  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, attribution_method, **attribution_kwargs)\n</code></pre> <p>Attributes model inputs for a selected concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences or a dataset.</p> required <code>int</code> <p>Index identifying the position of the concept of interest (score in the <code>ConceptsActivations</code> tensor) for which relevant input elements should be retrieved.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef input_concept_attribution(\n    self,\n    inputs: ModelInputs,\n    concept: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Attributes model inputs for a selected concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences\n            or a dataset.\n        concept (int): Index identifying the position of the concept of interest (score in the\n            `ConceptsActivations` tensor) for which relevant input elements should be retrieved.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    raise NotImplementedError(\"Input-to-concept attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.input_concept_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.concept_output_attribution","title":"concept_output_attribution","text":"<pre><code>concept_output_attribution(inputs, concepts, target, attribution_method, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each concept for the logit of a target output element.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>An input data-point for the model.</p> required <code>Tensor</code> <p>Concept activation tensor.</p> required <code>int</code> <p>The target class for which the concept output attribution should be computed.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each concept.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef concept_output_attribution(\n    self,\n    inputs: ModelInputs,\n    concepts: ConceptsActivations,\n    target: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Computes the attribution of each concept for the logit of a target output element.\n\n    Args:\n        inputs (ModelInputs): An input data-point for the model.\n        concepts (torch.Tensor): Concept activation tensor.\n        target (int): The target class for which the concept output attribution should be computed.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each concept.\n    \"\"\"\n    raise NotImplementedError(\"Concept-to-output attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.concept_output_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.concept_output_attribution(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.concept_output_attribution(target)","title":"<code>target</code>","text":""},{"location":"api/concepts/methods/base/#interpreto.concepts.ConceptAutoEncoderExplainer.concept_output_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/cockatiel/","title":"Cockatiel","text":"<p>Implementation of the COCKATIEL framework from COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP by Jourdan et al. (2023).</p>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel","title":"interpreto.concepts.Cockatiel","text":"<pre><code>Cockatiel(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', force_relu=False, **kwargs)\n</code></pre> <p>               Bases: <code>NMFConcepts</code></p> <p>Code:  <code>concepts/methods/cockatiel.py</code> </p> <p>Implementation of the Cockatiel concept explainer by Jourdan et al. (2023)<sup>1</sup>.</p> <ol> <li> <p>Jourdan F., Picard A., Fel T., Risser A., Loubes JM., and Asher N. COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP. Findings of the Association for Computational Linguistics (ACL 2023), pp. 5120\u20135136, 2023.\u00a0\u21a9</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. Default: <code>None</code>, set only when the concept explainer is fitted.</p> <code>concept_model</code> <code>SemiNMF</code> <p>An Overcomplete NMF encoder-decoder.</p> <code>force_relu</code> <code>bool</code> <p>Whether to force the activations to be positive.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <code>has_differentiable_concept_decoder</code> <code>bool</code> <p>Whether the <code>decode_concepts</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>int</code> <p>Size of the SAE concept space.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> <code>device | str</code> <p>Device to use for the <code>concept_module</code>.</p> <code>'cpu'</code> <code>bool</code> <p>Whether to force the activations to be positive.</p> <code>False</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_module</code>. See the Overcomplete documentation of the provided <code>concept_model_class</code> for more details.</p> <code>{}</code> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    *,\n    nb_concepts: int,\n    split_point: str | None = None,\n    device: torch.device | str = \"cpu\",\n    force_relu: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the concept bottleneck explainer based on the Overcomplete BaseOptimDictionaryLearning framework.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        nb_concepts (int): Size of the SAE concept space.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n        device (torch.device | str): Device to use for the `concept_module`.\n        force_relu (bool): Whether to force the activations to be positive.\n        **kwargs (dict): Additional keyword arguments to pass to the `concept_module`.\n            See the Overcomplete documentation of the provided `concept_model_class` for more details.\n    \"\"\"\n    super().__init__(\n        model_with_split_points,\n        nb_concepts=nb_concepts,\n        split_point=split_point,\n        device=device,\n        **kwargs,\n    )\n    self.force_relu = force_relu\n    self.has_differentiable_concept_encoder = False\n    self.has_differentiable_concept_decoder = True\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(nb_concepts)","title":"<code>nb_concepts</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(device)","title":"<code>device</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(force_relu)","title":"<code>force_relu</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.fit","title":"fit","text":"<pre><code>fit(activations, *, overwrite=False, **kwargs)\n</code></pre> <p>Fit an Overcomplete OptimDictionaryLearning model on the given activations.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor | dict[str, Tensor]</code> <p>The activations used for fitting the <code>concept_model</code>. If a dictionary is provided, the activation corresponding to <code>split_point</code> will be used.</p> required <code>bool</code> <p>Whether to overwrite the current model if it has already been fitted. Default: False.</p> <code>False</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_model</code>. See the Overcomplete documentation of the provided <code>concept_model</code> for more details.</p> <code>{}</code> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def fit(self, activations: LatentActivations | InterventionProxy, *, overwrite: bool = False, **kwargs):\n    \"\"\"Fit an Overcomplete OptimDictionaryLearning model on the given activations.\n\n    Args:\n        activations (torch.Tensor | dict[str, torch.Tensor]): The activations used for fitting the `concept_model`.\n            If a dictionary is provided, the activation corresponding to `split_point` will be used.\n        overwrite (bool): Whether to overwrite the current model if it has already been fitted.\n            Default: False.\n        **kwargs (dict): Additional keyword arguments to pass to the `concept_model`.\n            See the Overcomplete documentation of the provided `concept_model` for more details.\n    \"\"\"\n    split_activations = self._prepare_fit(activations, overwrite=overwrite)\n    if (split_activations &lt; 0).any():\n        if self.force_relu:\n            split_activations = torch.nn.functional.relu(split_activations)\n        else:\n            raise ValueError(\n                \"The activations should be positive. If you want to force the activations to be positive, \"\n                \"use the `NMFConcepts(..., force_relu=True)`.\"\n            )\n    self.concept_model.fit(split_activations, **kwargs)\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.fit(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.fit(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.fit(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.encode_activations","title":"encode_activations","text":"<pre><code>encode_activations(activations)\n</code></pre> <p>Encode the given activations using the <code>concept_model</code> encoder.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations</code> <p>The activations to encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded concept activations.</p> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>@check_fitted\ndef encode_activations(self, activations: LatentActivations) -&gt; torch.Tensor:  # ConceptsActivations\n    \"\"\"Encode the given activations using the `concept_model` encoder.\n\n    Args:\n        activations (LatentActivations): The activations to encode.\n\n    Returns:\n        The encoded concept activations.\n    \"\"\"\n    self._sanitize_activations(activations)\n    if (activations &lt; 0).any():\n        if self.force_relu:\n            activations = torch.nn.functional.relu(activations)\n        else:\n            raise ValueError(\n                \"The activations should be positive. If you want to force the activations to be positive, \"\n                \"use the `NMFConcepts(..., force_relu=True)`.\"\n            )\n    return self.concept_model.encode(activations)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.encode_activations(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.decode_concepts","title":"decode_concepts","text":"<pre><code>decode_concepts(concepts)\n</code></pre> <p>Decode the given concepts using the <code>concept_model</code> decoder.</p> <p>Parameters:</p> Name Type Description Default <code>ConceptsActivations</code> <p>The concepts to decode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The decoded model activations.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef decode_concepts(self, concepts: ConceptsActivations) -&gt; torch.Tensor:  # LatentActivations\n    \"\"\"Decode the given concepts using the `concept_model` decoder.\n\n    Args:\n        concepts (ConceptsActivations): The concepts to decode.\n\n    Returns:\n        The decoded model activations.\n    \"\"\"\n    return self.concept_model.decode(concepts)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.decode_concepts(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.get_dictionary","title":"get_dictionary","text":"<pre><code>get_dictionary()\n</code></pre> <p>Get the dictionary learned by the fitted <code>concept_model</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A <code>torch.Tensor</code> containing the learned dictionary.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef get_dictionary(self) -&gt; torch.Tensor:  # TODO: add this to tests\n    \"\"\"Get the dictionary learned by the fitted `concept_model`.\n\n    Returns:\n        torch.Tensor: A `torch.Tensor` containing the learned dictionary.\n    \"\"\"\n    return self.concept_model.get_dictionary()  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each input to a given concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences, or a dataset.</p> required <code>int | list[int]</code> <p>The concept index (or list of concepts indices) to analyze.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/methods/cockatiel.py</code> <pre><code>def input_concept_attribution(\n    self,\n    inputs: ModelInput,\n    concept: int,\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"\n    Computes the attribution of each input to a given concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences, or a dataset.\n        concept (int | list[int]): The concept index (or list of concepts indices) to analyze.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    return super().input_concept_attribution(\n        inputs, concept, \"Occlusion\", **attribution_kwargs\n    )  # TODO: add occlusion class when it exists\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.concept_output_attribution","title":"concept_output_attribution","text":"<pre><code>concept_output_attribution(inputs, concepts, target, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each concept for the logit of a target output element.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>An input datapoint for the model.</p> required <code>Tensor</code> <p>Concept activation tensor.</p> required <code>int</code> <p>The target class for which the concept output attribution should be computed.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each concept.</p> Source code in <code>interpreto/concepts/methods/cockatiel.py</code> <pre><code>def concept_output_attribution(\n    self, inputs: ModelInputs, concepts: ConceptsActivations, target: int, **attribution_kwargs\n) -&gt; list[float]:\n    \"\"\"Computes the attribution of each concept for the logit of a target output element.\n\n    Args:\n        inputs (ModelInputs): An input datapoint for the model.\n        concepts (torch.Tensor): Concept activation tensor.\n        target (int): The target class for which the concept output attribution should be computed.\n\n    Returns:\n        A list of attribution scores for each concept.\n    \"\"\"\n    return super().concept_output_attribution(\n        inputs, concepts, target, attribution_method=\"Sobol\", **attribution_kwargs\n    )  # TODO: add sobol class when it exists\n</code></pre>"},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.concept_output_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.concept_output_attribution(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/cockatiel/#interpreto.concepts.Cockatiel.concept_output_attribution(target)","title":"<code>target</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/","title":"Neurons as Concepts","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts","title":"interpreto.concepts.NeuronsAsConcepts","text":"<pre><code>NeuronsAsConcepts(model_with_split_points, split_point=None)\n</code></pre> <p>               Bases: <code>ConceptAutoEncoderExplainer</code></p> <p>Code:  <code>concepts/methods/neurons_as_concepts.py</code>  Concept Bottleneck Explainer where the latent space is considered as the concept space.</p>"},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts--todo-add-doc-with-papers-we-can-redo-with-it","title":"TODO: Add doc with papers we can redo with it.","text":"<p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str</code> <p>The split point used to train the <code>concept_model</code>.</p> <code>concept_model</code> <code>IdentityConceptModel</code> <p>An identity concept model for harmonization.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <code>has_differentiable_concept_decoder</code> <code>bool</code> <p>Whether the <code>decode_concepts</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> Source code in <code>interpreto/concepts/methods/neurons_as_concepts.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    split_point: str | None = None,\n):\n    \"\"\"\n    Initializes the concept explainer with a given splitted model.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n    \"\"\"\n    # extract the input size from the model activations\n    self.model_with_split_points = model_with_split_points\n    self.split_point: str = split_point  # type: ignore\n    input_size = self.model_with_split_points.get_latent_shape()[self.split_point][-1]\n\n    # initialize\n    super().__init__(\n        model_with_split_points=model_with_split_points,\n        concept_model=IdentityConceptModel(input_size),\n        split_point=self.split_point,\n    )\n    self.has_differentiable_concept_encoder = True\n    self.has_differentiable_concept_decoder = True\n</code></pre>"},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, attribution_method, **attribution_kwargs)\n</code></pre> <p>Attributes model inputs for a selected concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences or a dataset.</p> required <code>int</code> <p>Index identifying the position of the concept of interest (score in the <code>ConceptsActivations</code> tensor) for which relevant input elements should be retrieved.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef input_concept_attribution(\n    self,\n    inputs: ModelInputs,\n    concept: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Attributes model inputs for a selected concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences\n            or a dataset.\n        concept (int): Index identifying the position of the concept of interest (score in the\n            `ConceptsActivations` tensor) for which relevant input elements should be retrieved.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    raise NotImplementedError(\"Input-to-concept attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.input_concept_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.concept_output_attribution","title":"concept_output_attribution","text":"<pre><code>concept_output_attribution(inputs, concepts, target, attribution_method, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each concept for the logit of a target output element.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>An input data-point for the model.</p> required <code>Tensor</code> <p>Concept activation tensor.</p> required <code>int</code> <p>The target class for which the concept output attribution should be computed.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each concept.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef concept_output_attribution(\n    self,\n    inputs: ModelInputs,\n    concepts: ConceptsActivations,\n    target: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Computes the attribution of each concept for the logit of a target output element.\n\n    Args:\n        inputs (ModelInputs): An input data-point for the model.\n        concepts (torch.Tensor): Concept activation tensor.\n        target (int): The target class for which the concept output attribution should be computed.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each concept.\n    \"\"\"\n    raise NotImplementedError(\"Concept-to-output attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.concept_output_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.concept_output_attribution(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.concept_output_attribution(target)","title":"<code>target</code>","text":""},{"location":"api/concepts/methods/neurons_as_concepts/#interpreto.concepts.NeuronsAsConcepts.concept_output_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/optim/","title":"Optimization-based Dictionary Learning","text":""},{"location":"api/concepts/methods/optim/#list-of-available-methods","title":"List of available methods","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.NMFConcepts","title":"interpreto.concepts.NMFConcepts","text":"<pre><code>NMFConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', force_relu=False, **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[NMF]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the NMF from Lee and Seung (1999)<sup>1</sup> as concept model.</p> <p>NMF implementation from overcomplete.optimization.NMF class.</p> <ol> <li> <p>Lee, D., Seung, H. Learning the parts of objects by non-negative matrix factorization. Nature, 401, 1999, pp. 788\u2013791.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>nb_concepts</code> <code>int</code> <p>Size of the SAE concept space.</p> required <code>split_point</code> <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> <code>device</code> <code>device | str</code> <p>Device to use for the <code>concept_module</code>.</p> <code>'cpu'</code> <code>force_relu</code> <code>bool</code> <p>Whether to force the activations to be positive.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_module</code>. See the Overcomplete documentation of the provided <code>concept_model_class</code> for more details.</p> <code>{}</code>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.NMFConcepts.encode_activations","title":"encode_activations","text":"<pre><code>encode_activations(activations)\n</code></pre> <p>Encode the given activations using the <code>concept_model</code> encoder.</p> <p>Parameters:</p> Name Type Description Default <code>activations</code> <code>LatentActivations</code> <p>The activations to encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded concept activations.</p>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.SemiNMFConcepts","title":"interpreto.concepts.SemiNMFConcepts","text":"<pre><code>SemiNMFConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SemiNMF]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the SemiNMF from Ding et al. (2008)<sup>1</sup> as concept model.</p> <p>SemiNMF implementation from overcomplete.optimization.SemiNMF class.</p> <ol> <li> <p>C. H. Q. Ding, T. Li and M. I. Jordan, Convex and Semi-Nonnegative Matrix Factorizations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1), 2010, pp. 45-55\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.ConvexNMFConcepts","title":"interpreto.concepts.ConvexNMFConcepts","text":"<pre><code>ConvexNMFConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[ConvexNMF]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the ConvexNMF from Ding et al. (2008)<sup>1</sup> as concept model.</p> <p>ConvexNMF implementation from overcomplete.optimization.ConvexNMF class.</p> <ol> <li> <p>C. H. Q. Ding, T. Li and M. I. Jordan, Convex and Semi-Nonnegative Matrix Factorizations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1), 2010, pp. 45-55\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>nb_concepts</code> <code>int</code> <p>Size of the SAE concept space.</p> required <code>split_point</code> <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> <code>device</code> <code>device | str</code> <p>Device to use for the <code>concept_module</code>.</p> <code>'cpu'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_module</code>. See the Overcomplete documentation of the provided <code>concept_model_class</code> for more details.</p> <code>{}</code>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.PCAConcepts","title":"interpreto.concepts.PCAConcepts","text":"<pre><code>PCAConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkPCA]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the PCA from Pearson (1901)<sup>1</sup> as concept model.</p> <p>PCA implementation from overcomplete.optimization.SkPCA class.</p> <ol> <li> <p>K. Pearson, On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2(11), 1901, pp. 559-572.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.ICAConcepts","title":"interpreto.concepts.ICAConcepts","text":"<pre><code>ICAConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkICA]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the ICA from Hyvarinen and Oja (2000)<sup>1</sup> as concept model.</p> <p>ICA implementation from overcomplete.optimization.SkICA class.</p> <ol> <li> <p>A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.KMeansConcepts","title":"interpreto.concepts.KMeansConcepts","text":"<pre><code>KMeansConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkKMeans]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the K-Means as concept model.</p> <p>K-Means implementation from overcomplete.optimization.SkKMeans class.</p>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.SparsePCAConcepts","title":"interpreto.concepts.SparsePCAConcepts","text":"<pre><code>SparsePCAConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkSparsePCA]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with SparsePCA as concept model.</p> <p>SparsePCA implementation from overcomplete.optimization.SkSparsePCA class.</p>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.SVDConcepts","title":"interpreto.concepts.SVDConcepts","text":"<pre><code>SVDConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkSVD]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with SVD as concept model.</p> <p>SVD implementation from overcomplete.optimization.SkSVD class.</p>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.DictionaryLearningConcepts","title":"interpreto.concepts.DictionaryLearningConcepts","text":"<pre><code>DictionaryLearningConcepts(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>DictionaryLearningExplainer[SkDictionaryLearning]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the Dictionary Learning concepts from Mairal et al. (2009)<sup>1</sup> as concept model.</p> <p>Dictionary Learning implementation from overcomplete.optimization.SkDictionaryLearning class.</p> <ol> <li> <p>J. Mairal, F. Bach, J. Ponce, G. Sapiro, Online dictionary learning for sparse coding Proceedings of the 26th Annual International Conference on Machine Learning, 2009, pp. 689-696.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/optim/#base-abstract-class","title":"Base abstract class","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer","title":"interpreto.concepts.methods.DictionaryLearningExplainer","text":"<pre><code>DictionaryLearningExplainer(model_with_split_points, *, nb_concepts, split_point=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>ConceptAutoEncoderExplainer[BaseOptimDictionaryLearning]</code>, <code>Generic[_BODL_co]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p>Implementation of a concept explainer using an overcomplete.optimization.BaseOptimDictionaryLearning     (NMF and PCA variants) as <code>concept_model</code>.</p> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. Default: <code>None</code>, set only when the concept explainer is fitted.</p> <code>concept_model</code> <code>SAE</code> <p>An Overcomplete BaseOptimDictionaryLearning variant for concept extraction.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <code>has_differentiable_concept_decoder</code> <code>bool</code> <p>Whether the <code>decode_concepts</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>int</code> <p>Size of the SAE concept space.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> <code>device | str</code> <p>Device to use for the <code>concept_module</code>.</p> <code>'cpu'</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_module</code>. See the Overcomplete documentation of the provided <code>concept_model_class</code> for more details.</p> <code>{}</code> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    *,\n    nb_concepts: int,\n    split_point: str | None = None,\n    device: torch.device | str = \"cpu\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize the concept bottleneck explainer based on the Overcomplete BaseOptimDictionaryLearning framework.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        nb_concepts (int): Size of the SAE concept space.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n        device (torch.device | str): Device to use for the `concept_module`.\n        **kwargs (dict): Additional keyword arguments to pass to the `concept_module`.\n            See the Overcomplete documentation of the provided `concept_model_class` for more details.\n    \"\"\"\n    concept_model = self.concept_model_class(\n        nb_concepts=nb_concepts,\n        device=device,  # type: ignore\n        **kwargs,\n    )\n    super().__init__(model_with_split_points, concept_model, split_point)\n    self.has_differentiable_concept_encoder = True\n    self.has_differentiable_concept_decoder = True\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer(nb_concepts)","title":"<code>nb_concepts</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer(device)","title":"<code>device</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.fit","title":"fit","text":"<pre><code>fit(activations, *, overwrite=False, **kwargs)\n</code></pre> <p>Fit an Overcomplete OptimDictionaryLearning model on the given activations.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor | dict[str, Tensor]</code> <p>The activations used for fitting the <code>concept_model</code>. If a dictionary is provided, the activation corresponding to <code>split_point</code> will be used.</p> required <code>bool</code> <p>Whether to overwrite the current model if it has already been fitted. Default: False.</p> <code>False</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_model</code>. See the Overcomplete documentation of the provided <code>concept_model</code> for more details.</p> <code>{}</code> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def fit(self, activations: LatentActivations | InterventionProxy, *, overwrite: bool = False, **kwargs):\n    \"\"\"Fit an Overcomplete OptimDictionaryLearning model on the given activations.\n\n    Args:\n        activations (torch.Tensor | dict[str, torch.Tensor]): The activations used for fitting the `concept_model`.\n            If a dictionary is provided, the activation corresponding to `split_point` will be used.\n        overwrite (bool): Whether to overwrite the current model if it has already been fitted.\n            Default: False.\n        **kwargs (dict): Additional keyword arguments to pass to the `concept_model`.\n            See the Overcomplete documentation of the provided `concept_model` for more details.\n    \"\"\"\n    split_activations = self._prepare_fit(activations, overwrite=overwrite)\n    self.concept_model.fit(split_activations, **kwargs)\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.fit(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.fit(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.fit(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.encode_activations","title":"encode_activations","text":"<pre><code>encode_activations(activations)\n</code></pre> <p>Encode the given activations using the <code>concept_model</code> encoder.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations</code> <p>The activations to encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded concept activations.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef encode_activations(self, activations: LatentActivations) -&gt; torch.Tensor:  # ConceptsActivations\n    \"\"\"Encode the given activations using the `concept_model` encoder.\n\n    Args:\n        activations (LatentActivations): The activations to encode.\n\n    Returns:\n        The encoded concept activations.\n    \"\"\"\n    self._sanitize_activations(activations)\n    return self.concept_model.encode(activations)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.encode_activations(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.decode_concepts","title":"decode_concepts","text":"<pre><code>decode_concepts(concepts)\n</code></pre> <p>Decode the given concepts using the <code>concept_model</code> decoder.</p> <p>Parameters:</p> Name Type Description Default <code>ConceptsActivations</code> <p>The concepts to decode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The decoded model activations.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef decode_concepts(self, concepts: ConceptsActivations) -&gt; torch.Tensor:  # LatentActivations\n    \"\"\"Decode the given concepts using the `concept_model` decoder.\n\n    Args:\n        concepts (ConceptsActivations): The concepts to decode.\n\n    Returns:\n        The decoded model activations.\n    \"\"\"\n    return self.concept_model.decode(concepts)  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.decode_concepts(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.get_dictionary","title":"get_dictionary","text":"<pre><code>get_dictionary()\n</code></pre> <p>Get the dictionary learned by the fitted <code>concept_model</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A <code>torch.Tensor</code> containing the learned dictionary.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef get_dictionary(self) -&gt; torch.Tensor:  # TODO: add this to tests\n    \"\"\"Get the dictionary learned by the fitted `concept_model`.\n\n    Returns:\n        torch.Tensor: A `torch.Tensor` containing the learned dictionary.\n    \"\"\"\n    return self.concept_model.get_dictionary()  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, attribution_method, **attribution_kwargs)\n</code></pre> <p>Attributes model inputs for a selected concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences or a dataset.</p> required <code>int</code> <p>Index identifying the position of the concept of interest (score in the <code>ConceptsActivations</code> tensor) for which relevant input elements should be retrieved.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef input_concept_attribution(\n    self,\n    inputs: ModelInputs,\n    concept: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Attributes model inputs for a selected concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences\n            or a dataset.\n        concept (int): Index identifying the position of the concept of interest (score in the\n            `ConceptsActivations` tensor) for which relevant input elements should be retrieved.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    raise NotImplementedError(\"Input-to-concept attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.input_concept_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.concept_output_attribution","title":"concept_output_attribution","text":"<pre><code>concept_output_attribution(inputs, concepts, target, attribution_method, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each concept for the logit of a target output element.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>An input data-point for the model.</p> required <code>Tensor</code> <p>Concept activation tensor.</p> required <code>int</code> <p>The target class for which the concept output attribution should be computed.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each concept.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef concept_output_attribution(\n    self,\n    inputs: ModelInputs,\n    concepts: ConceptsActivations,\n    target: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Computes the attribution of each concept for the logit of a target output element.\n\n    Args:\n        inputs (ModelInputs): An input data-point for the model.\n        concepts (torch.Tensor): Concept activation tensor.\n        target (int): The target class for which the concept output attribution should be computed.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each concept.\n    \"\"\"\n    raise NotImplementedError(\"Concept-to-output attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.concept_output_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.concept_output_attribution(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.concept_output_attribution(target)","title":"<code>target</code>","text":""},{"location":"api/concepts/methods/optim/#interpreto.concepts.methods.DictionaryLearningExplainer.concept_output_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/sae/","title":"Sparse Autoencoders (SAEs)","text":""},{"location":"api/concepts/methods/sae/#list-of-available-saes","title":"List of available SAEs","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.VanillaSAEConcepts","title":"interpreto.concepts.methods.VanillaSAEConcepts","text":"<pre><code>VanillaSAEConcepts(model_with_split_points, *, nb_concepts, split_point=None, encoder_module=None, dictionary_params=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>SAEExplainer[SAE]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the Vanilla SAE from Cunningham et al. (2023)<sup>1</sup> and Bricken et al. (2023)<sup>2</sup> as concept model.</p> <p>Vanilla SAE implementation from overcomplete.sae.SAE class.</p> <ol> <li> <p>Huben, R., Cunningham, H., Smith, L. R., Ewart, A., Sharkey, L. Sparse Autoencoders Find Highly Interpretable Features in Language Models. The Twelfth International Conference on Learning Representations, 2024.\u00a0\u21a9</p> </li> <li> <p>Bricken, T. et al., Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, Transformer Circuits Thread, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.TopKSAEConcepts","title":"interpreto.concepts.methods.TopKSAEConcepts","text":"<pre><code>TopKSAEConcepts(model_with_split_points, *, nb_concepts, split_point=None, encoder_module=None, dictionary_params=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>SAEExplainer[TopKSAE]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the TopK SAE from Gao et al. (2024)<sup>1</sup> as concept model.</p> <p>TopK SAE implementation from overcomplete.sae.TopKSAE class.</p> <ol> <li> <p>Gao, L. et al., Scaling and evaluating sparse autoencoders. The Thirteenth International Conference on Learning Representations, 2025.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.BatchTopKSAEConcepts","title":"interpreto.concepts.methods.BatchTopKSAEConcepts","text":"<pre><code>BatchTopKSAEConcepts(model_with_split_points, *, nb_concepts, split_point=None, encoder_module=None, dictionary_params=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>SAEExplainer[BatchTopKSAE]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the BatchTopK SAE from Bussmann et al. (2024)<sup>1</sup> as concept model.</p> <p>BatchTopK SAE implementation from overcomplete.sae.BatchTopKSAE class.</p> <ol> <li> <p>Bussmann, B., Leask, P., Nanda, N. BatchTopK Sparse Autoencoders. Arxiv Preprint, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.JumpReLUSAEConcepts","title":"interpreto.concepts.methods.JumpReLUSAEConcepts","text":"<pre><code>JumpReLUSAEConcepts(model_with_split_points, *, nb_concepts, split_point=None, encoder_module=None, dictionary_params=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>SAEExplainer[JumpSAE]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p><code>ConceptAutoEncoderExplainer</code> with the JumpReLU SAE from Rajamanoharan et al. (2024)<sup>1</sup> as concept model.</p> <p>JumpReLU SAE implementation from overcomplete.sae.JumpReLUSAE class.</p> <ol> <li> <p>Rajamanoharan, S. et al., Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders. Arxiv Preprint, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/concepts/methods/sae/#abstract-base-class","title":"Abstract base class","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer","title":"interpreto.concepts.methods.SAEExplainer","text":"<pre><code>SAEExplainer(model_with_split_points, *, nb_concepts, split_point=None, encoder_module=None, dictionary_params=None, device='cpu', **kwargs)\n</code></pre> <p>               Bases: <code>ConceptAutoEncoderExplainer[SAE]</code>, <code>Generic[_SAE_co]</code></p> <p>Code:  <code>concepts/methods/overcomplete.py</code> </p> <p>Implementation of a concept explainer using a overcomplete.sae.SAE variant as <code>concept_model</code>.</p> <p>Attributes:</p> Name Type Description <code>model_with_split_points</code> <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which <code>concept_model</code> can be fitted.</p> <code>split_point</code> <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. Default: <code>None</code>, set only when the concept explainer is fitted.</p> <code>concept_model</code> <code>SAE</code> <p>An Overcomplete SAE variant for concept extraction.</p> <code>is_fitted</code> <code>bool</code> <p>Whether the <code>concept_model</code> was fit on model activations.</p> <code>has_differentiable_concept_encoder</code> <code>bool</code> <p>Whether the <code>encode_activations</code> operation is differentiable.</p> <code>has_differentiable_concept_decoder</code> <code>bool</code> <p>Whether the <code>decode_concepts</code> operation is differentiable.</p> <p>Parameters:</p> Name Type Description Default <code>ModelWithSplitPoints</code> <p>The model to apply the explanation on. It should have at least one split point on which a concept explainer can be trained.</p> required <code>int</code> <p>Size of the SAE concept space.</p> required <code>str | None</code> <p>The split point used to train the <code>concept_model</code>. If None, tries to use the split point of <code>model_with_split_points</code> if a single one is defined.</p> <code>None</code> <code>Module | str | None</code> <p>Encoder module to use to construct the SAE, see Overcomplete SAE documentation.</p> <code>None</code> <code>dict | None</code> <p>Dictionary parameters to use to construct the SAE, see Overcomplete SAE documentation.</p> <code>None</code> <code>device | str</code> <p>Device to use for the <code>concept_module</code>.</p> <code>'cpu'</code> <code>dict</code> <p>Additional keyword arguments to pass to the <code>concept_module</code>. See the Overcomplete documentation of the provided <code>concept_model_class</code> for more details.</p> <code>{}</code> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def __init__(\n    self,\n    model_with_split_points: ModelWithSplitPoints,\n    *,\n    nb_concepts: int,\n    split_point: str | None = None,\n    encoder_module: nn.Module | str | None = None,\n    dictionary_params: dict | None = None,\n    device: str = \"cpu\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize the concept bottleneck explainer based on the Overcomplete SAE framework.\n\n    Args:\n        model_with_split_points (ModelWithSplitPoints): The model to apply the explanation on.\n            It should have at least one split point on which a concept explainer can be trained.\n        nb_concepts (int): Size of the SAE concept space.\n        split_point (str | None): The split point used to train the `concept_model`. If None, tries to use the\n            split point of `model_with_split_points` if a single one is defined.\n        encoder_module (nn.Module | str | None): Encoder module to use to construct the SAE, see [Overcomplete SAE documentation](https://kempnerinstitute.github.io/overcomplete/saes/vanilla/).\n        dictionary_params (dict | None): Dictionary parameters to use to construct the SAE, see [Overcomplete SAE documentation](https://kempnerinstitute.github.io/overcomplete/saes/vanilla/).\n        device (torch.device | str): Device to use for the `concept_module`.\n        **kwargs (dict): Additional keyword arguments to pass to the `concept_module`.\n            See the Overcomplete documentation of the provided `concept_model_class` for more details.\n    \"\"\"\n    if not issubclass(self.concept_model_class, oc_sae.SAE):\n        raise ValueError(\n            \"ConceptEncoderDecoder must be a subclass of `overcomplete.sae.SAE`.\\n\"\n            \"Use `interpreto.concepts.methods.SAEExplainerClasses` to get the list of available SAE methods.\"\n        )\n    self.model_with_split_points = model_with_split_points\n    self.split_point: str = split_point  # type: ignore\n\n    # TODO: this will be replaced with a scan and a better way to select how to pick activations based on model class\n    shapes = self.model_with_split_points.get_latent_shape()\n    concept_model = self.concept_model_class(\n        input_shape=shapes[self.split_point][-1],\n        nb_concepts=nb_concepts,\n        encoder_module=encoder_module,\n        dictionary_params=dictionary_params,\n        device=device,\n        **kwargs,\n    )\n    super().__init__(model_with_split_points, concept_model, self.split_point)\n    self.has_differentiable_concept_encoder = True\n    self.has_differentiable_concept_decoder = True\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(model_with_split_points)","title":"<code>model_with_split_points</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(nb_concepts)","title":"<code>nb_concepts</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(split_point)","title":"<code>split_point</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(encoder_module)","title":"<code>encoder_module</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(dictionary_params)","title":"<code>dictionary_params</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(device)","title":"<code>device</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit","title":"fit","text":"<pre><code>fit(activations, *, use_amp=False, batch_size=1024, criterion=MSELoss, optimizer_class=Adam, scheduler_class=None, lr=0.001, nb_epochs=20, clip_grad=None, monitoring=None, device='cpu', max_nan_fallbacks=5, overwrite=False)\n</code></pre> <p>Fit an Overcomplete SAE model on the given activations.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor | dict[str, Tensor]</code> <p>The activations used for fitting the <code>concept_model</code>. If a dictionary is provided, the activation corresponding to <code>split_point</code> will be used.</p> required <code>bool</code> <p>Whether to use automatic mixed precision for fitting.</p> <code>False</code> <code>SAELoss</code> <p>Loss criterion for the training of the <code>concept_model</code>.</p> <code>MSELoss</code> <code>type[Optimizer]</code> <p>Optimizer for the training of the <code>concept_model</code>.</p> <code>Adam</code> <code>type[LRScheduler] | None</code> <p>Learning rate scheduler for the training of the <code>concept_model</code>.</p> <code>None</code> <code>float</code> <p>Learning rate for the training of the <code>concept_model</code>.</p> <code>0.001</code> <code>int</code> <p>Number of epochs for the training of the <code>concept_model</code>.</p> <code>20</code> <code>float | None</code> <p>Gradient clipping value for the training of the <code>concept_model</code>.</p> <code>None</code> <code>int | None</code> <p>Monitoring frequency for the training of the <code>concept_model</code>.</p> <code>None</code> <code>device | str</code> <p>Device to use for the training of the <code>concept_model</code>.</p> <code>'cpu'</code> <code>int | None</code> <p>Maximum number of fallbacks to use when NaNs are encountered during training. Ignored if use_amp is False.</p> <code>5</code> <code>bool</code> <p>Whether to overwrite the current model if it has already been fitted. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with training history logs.</p> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>def fit(\n    self,\n    activations: LatentActivations | InterventionProxy,\n    *,\n    use_amp: bool = False,\n    batch_size: int = 1024,\n    criterion: type[SAELoss] = MSELoss,\n    optimizer_class: type[torch.optim.Optimizer] = torch.optim.Adam,\n    scheduler_class: type[torch.optim.lr_scheduler.LRScheduler] | None = None,\n    lr: float = 1e-3,\n    nb_epochs: int = 20,\n    clip_grad: float | None = None,\n    monitoring: int | None = None,\n    device: torch.device | str = \"cpu\",\n    max_nan_fallbacks: int | None = 5,\n    overwrite: bool = False,\n) -&gt; dict:\n    \"\"\"Fit an Overcomplete SAE model on the given activations.\n\n    Args:\n        activations (torch.Tensor | dict[str, torch.Tensor]): The activations used for fitting the `concept_model`.\n            If a dictionary is provided, the activation corresponding to `split_point` will be used.\n        use_amp (bool): Whether to use automatic mixed precision for fitting.\n        criterion (interpreto.concepts.SAELoss): Loss criterion for the training of the `concept_model`.\n        optimizer_class (type[torch.optim.Optimizer]): Optimizer for the training of the `concept_model`.\n        scheduler_class (type[torch.optim.lr_scheduler.LRScheduler] | None): Learning rate scheduler for the\n            training of the `concept_model`.\n        lr (float): Learning rate for the training of the `concept_model`.\n        nb_epochs (int): Number of epochs for the training of the `concept_model`.\n        clip_grad (float | None): Gradient clipping value for the training of the `concept_model`.\n        monitoring (int | None): Monitoring frequency for the training of the `concept_model`.\n        device (torch.device | str): Device to use for the training of the `concept_model`.\n        max_nan_fallbacks (int | None): Maximum number of fallbacks to use when NaNs are encountered during\n            training. Ignored if use_amp is False.\n        overwrite (bool): Whether to overwrite the current model if it has already been fitted.\n            Default: False.\n\n    Returns:\n        A dictionary with training history logs.\n    \"\"\"\n    split_activations = self._prepare_fit(activations, overwrite=overwrite)\n    dataloader = DataLoader(TensorDataset(split_activations.detach()), batch_size=batch_size, shuffle=True)\n    optimizer_kwargs = {\"lr\": lr}\n    optimizer = optimizer_class(self.concept_model.parameters(), **optimizer_kwargs)  # type: ignore\n    train_params = {\n        \"model\": self.concept_model,\n        \"dataloader\": dataloader,\n        \"criterion\": criterion(),\n        \"optimizer\": optimizer,\n        \"nb_epochs\": nb_epochs,\n        \"clip_grad\": clip_grad,\n        \"monitoring\": monitoring,\n        \"device\": device,\n    }\n    if scheduler_class is not None:\n        scheduler = scheduler_class(optimizer)\n        train_params[\"scheduler\"] = scheduler\n\n    if use_amp:\n        train_method = oc_sae.train.train_sae_amp\n        train_params[\"max_nan_fallbacks\"] = max_nan_fallbacks\n    else:\n        train_method = oc_sae.train_sae\n    log = train_method(**train_params)\n    self.concept_model.fitted = True\n    return log\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(use_amp)","title":"<code>use_amp</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(criterion)","title":"<code>criterion</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(optimizer_class)","title":"<code>optimizer_class</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(scheduler_class)","title":"<code>scheduler_class</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(lr)","title":"<code>lr</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(nb_epochs)","title":"<code>nb_epochs</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(clip_grad)","title":"<code>clip_grad</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(monitoring)","title":"<code>monitoring</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(device)","title":"<code>device</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(max_nan_fallbacks)","title":"<code>max_nan_fallbacks</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.fit(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.encode_activations","title":"encode_activations","text":"<pre><code>encode_activations(activations)\n</code></pre> <p>Encode the given activations using the <code>concept_model</code> encoder.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>The activations to encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The encoded concept activations.</p> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>@check_fitted\ndef encode_activations(self, activations: LatentActivations) -&gt; torch.Tensor:  # ConceptsActivations\n    \"\"\"Encode the given activations using the `concept_model` encoder.\n\n    Args:\n        activations (torch.Tensor): The activations to encode.\n\n    Returns:\n        The encoded concept activations.\n    \"\"\"\n    # SAEs.encode returns both codes (concepts activations) and pre_codes (before relu)\n    _, codes = super().encode_activations(activations.to(self.device))\n    return codes\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.encode_activations(activations)","title":"<code>activations</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.decode_concepts","title":"decode_concepts","text":"<pre><code>decode_concepts(concepts)\n</code></pre> <p>Decode the given concepts using the <code>concept_model</code> decoder.</p> <p>Parameters:</p> Name Type Description Default <code>Tensor</code> <p>The concepts to decode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The decoded concept activations.</p> Source code in <code>interpreto/concepts/methods/overcomplete.py</code> <pre><code>@check_fitted\ndef decode_concepts(self, concepts: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode the given concepts using the `concept_model` decoder.\n\n    Args:\n        concepts (torch.Tensor): The concepts to decode.\n\n    Returns:\n        The decoded concept activations.\n    \"\"\"\n    return self.concept_model.decode(concepts.to(self.device))  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.decode_concepts(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.get_dictionary","title":"get_dictionary","text":"<pre><code>get_dictionary()\n</code></pre> <p>Get the dictionary learned by the fitted <code>concept_model</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A <code>torch.Tensor</code> containing the learned dictionary.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef get_dictionary(self) -&gt; torch.Tensor:  # TODO: add this to tests\n    \"\"\"Get the dictionary learned by the fitted `concept_model`.\n\n    Returns:\n        torch.Tensor: A `torch.Tensor` containing the learned dictionary.\n    \"\"\"\n    return self.concept_model.get_dictionary()  # type: ignore\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.interpret","title":"interpret","text":"<pre><code>interpret(interpretation_method, concepts_indices, inputs=None, latent_activations=None, concepts_activations=None, **kwargs)\n</code></pre> <p>Interpret the concepts dimensions in the latent space into a human-readable format. The interpretation is a mapping between the concepts indices and an object allowing to interpret them. It can be a label, a description, examples, etc.</p> <p>Parameters:</p> Name Type Description Default <code>type[BaseConceptInterpretationMethod]</code> <p>The interpretation method to use to interpret the concepts.</p> required <code>int | list[int] | Literal['all']</code> <p>The indices of the concepts to interpret. If \"all\", all concepts are interpreted.</p> required <p>Additional keyword arguments to pass to the interpretation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[int, Any]</code> <p>Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef interpret(\n    self,\n    interpretation_method: type[BaseConceptInterpretationMethod],\n    concepts_indices: int | list[int] | Literal[\"all\"],\n    inputs: list[str] | None = None,\n    latent_activations: InterventionProxy | LatentActivations | None = None,\n    concepts_activations: ConceptsActivations | None = None,\n    **kwargs,\n) -&gt; Mapping[int, Any]:\n    \"\"\"\n    Interpret the concepts dimensions in the latent space into a human-readable format.\n    The interpretation is a mapping between the concepts indices and an object allowing to interpret them.\n    It can be a label, a description, examples, etc.\n\n    Args:\n        interpretation_method: The interpretation method to use to interpret the concepts.\n        concepts_indices (int | list[int] | Literal[\"all\"]): The indices of the concepts to interpret.\n            If \"all\", all concepts are interpreted.\n        **kwargs: Additional keyword arguments to pass to the interpretation method.\n\n    Returns:\n        Mapping[int, Any]: A mapping between the concepts indices and the interpretation of the concepts.\n    \"\"\"\n    if concepts_indices == \"all\":\n        concepts_indices = list(range(self.concept_model.nb_concepts))\n\n    # verify\n    if latent_activations is not None:\n        split_latent_activations = self._sanitize_activations(latent_activations)\n    else:\n        split_latent_activations = None\n\n    # initialize the interpretation method\n    method = interpretation_method(\n        model_with_split_points=self.model_with_split_points,\n        split_point=self.split_point,\n        concept_model=self.concept_model,\n        **kwargs,\n    )\n\n    # compute the interpretation from inputs and activations\n    return method.interpret(\n        concepts_indices=concepts_indices,\n        inputs=inputs,\n        latent_activations=split_latent_activations,\n        concepts_activations=concepts_activations,\n    )\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.interpret(interpretation_method)","title":"<code>interpretation_method</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.interpret(concepts_indices)","title":"<code>concepts_indices</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.interpret(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.input_concept_attribution","title":"input_concept_attribution","text":"<pre><code>input_concept_attribution(inputs, concept, attribution_method, **attribution_kwargs)\n</code></pre> <p>Attributes model inputs for a selected concept.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>The input data, which can be a string, a list of tokens/words/clauses/sentences or a dataset.</p> required <code>int</code> <p>Index identifying the position of the concept of interest (score in the <code>ConceptsActivations</code> tensor) for which relevant input elements should be retrieved.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each input.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef input_concept_attribution(\n    self,\n    inputs: ModelInputs,\n    concept: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Attributes model inputs for a selected concept.\n\n    Args:\n        inputs (ModelInputs): The input data, which can be a string, a list of tokens/words/clauses/sentences\n            or a dataset.\n        concept (int): Index identifying the position of the concept of interest (score in the\n            `ConceptsActivations` tensor) for which relevant input elements should be retrieved.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each input.\n    \"\"\"\n    raise NotImplementedError(\"Input-to-concept attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.input_concept_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.input_concept_attribution(concept)","title":"<code>concept</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.input_concept_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.concept_output_attribution","title":"concept_output_attribution","text":"<pre><code>concept_output_attribution(inputs, concepts, target, attribution_method, **attribution_kwargs)\n</code></pre> <p>Computes the attribution of each concept for the logit of a target output element.</p> <p>Parameters:</p> Name Type Description Default <code>ModelInputs</code> <p>An input data-point for the model.</p> required <code>Tensor</code> <p>Concept activation tensor.</p> required <code>int</code> <p>The target class for which the concept output attribution should be computed.</p> required <code>type[AttributionExplainer]</code> <p>The attribution method to obtain importance scores for input elements.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>A list of attribution scores for each concept.</p> Source code in <code>interpreto/concepts/base.py</code> <pre><code>@check_fitted\ndef concept_output_attribution(\n    self,\n    inputs: ModelInputs,\n    concepts: ConceptsActivations,\n    target: int,\n    attribution_method: type[AttributionExplainer],\n    **attribution_kwargs,\n) -&gt; list[float]:\n    \"\"\"Computes the attribution of each concept for the logit of a target output element.\n\n    Args:\n        inputs (ModelInputs): An input data-point for the model.\n        concepts (torch.Tensor): Concept activation tensor.\n        target (int): The target class for which the concept output attribution should be computed.\n        attribution_method: The attribution method to obtain importance scores for input elements.\n\n    Returns:\n        A list of attribution scores for each concept.\n    \"\"\"\n    raise NotImplementedError(\"Concept-to-output attribution method is not implemented yet.\")\n</code></pre>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.concept_output_attribution(inputs)","title":"<code>inputs</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.concept_output_attribution(concepts)","title":"<code>concepts</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.concept_output_attribution(target)","title":"<code>target</code>","text":""},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAEExplainer.concept_output_attribution(attribution_method)","title":"<code>attribution_method</code>","text":""},{"location":"api/concepts/methods/sae/#loss-functions","title":"Loss Functions","text":"<p>These functions can be passed as the <code>criterion</code> argument in the <code>fit</code> method of the <code>SAEExplainer</code> class. <code>MSELoss</code> is the default loss function.</p>"},{"location":"api/concepts/methods/sae/#interpreto.concepts.methods.SAELossClasses","title":"interpreto.concepts.methods.SAELossClasses","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of possible loss functions for SAEs.</p> <p>To pass as the <code>criterion</code> parameter of <code>SAEExplainer.fit()</code>.</p> <p>Attributes:</p> Name Type Description <code>MSE</code> <code>type[SAELoss]</code> <p>Mean Squared Error loss.</p> <code>DeadNeuronsReanimation</code> <code>type[SAELoss]</code> <p>Loss function promoting reanimation of dead neurons.</p>"},{"location":"api/concepts/metrics/dictionary_metrics/","title":"Concepts Dictionary Metrics","text":"<p>Concept dictionary metrics evaluate the concept space via its dictionary. They can either be compute on a single dictionary or compare two or more dictionaries.</p> <pre><code>from interpreto.concepts.metrics import MetricClass\n\nmetric = MetricClass(concept_explainer1, concept_explainer2, ...)\nscore = metric.compute()\n</code></pre>"},{"location":"api/concepts/metrics/dictionary_metrics/#stability","title":"Stability","text":""},{"location":"api/concepts/metrics/dictionary_metrics/#interpreto.concepts.metrics.Stability","title":"interpreto.concepts.metrics.Stability","text":"<pre><code>Stability(*concept_explainers, matching_algorithm=COSINE_HUNGARIAN)\n</code></pre> <p>Code  <code>concepts/metrics/dictionary_metrics.py</code></p> <p>Stability metric between sets of dictionaries, introduced by Fel et al. (2023)<sup>1</sup>. Also called Consistency by Paulo et Belrose (2025)<sup>2</sup>.</p> <ul> <li>If only one dictionary is provided, the metric is a self comparison of the dictionary.</li> <li>If two dictionaries are provided, the metric is a comparison between the two dictionaries.</li> <li>If more than two dictionaries are provided, the metric is the mean of the pairwise comparisons.</li> </ul> <ol> <li> <p>Fel, T., Boutin, V., B\u00e9thune, L., Cad\u00e8ne, R., Moayeri, M., And\u00e9ol, L., Chavidal, M., &amp; Serre, T. A holistic approach to unifying automatic concept extraction and concept importance estimation. Advances in Neural Information Processing Systems. 2023.\u00a0\u21a9</p> </li> <li> <p>Paulo, G et Belrose, N. Sparse Autoencoders Trained on the Same Data Learn Different Features 2025.\u00a0\u21a9</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>ConceptAutoEncoderExplainer | Float[Tensor, 'cpt d']</code> <p>The <code>ConceptAutoEncoderExplainer</code>s or dictionaries to compare. Both types are supported and can be mixed.</p> <code>()</code> <code>DistanceFunctionProtocol</code> <p>The algorithm used to match concepts between dictionaries. Defaults to ConceptMatchingAlgorithm.COSINE_HUNGARIAN.</p> <code>COSINE_HUNGARIAN</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from interpreto.concepts import NMFConcepts\n&gt;&gt;&gt; from interpreto.concepts.metrics import Stability\n</code></pre> <pre><code>&gt;&gt;&gt; # Iterate on random seeds\n&gt;&gt;&gt; concept_explainers = []\n&gt;&gt;&gt; for seed in range(10):\n...     # set seed\n...     torch.manual_seed(seed)\n...     # Create a concept model\n...     nmf_explainer = NMFConcepts(model_with_split_points, nb_concepts=20, device=\"cuda\", force_relu=True)\n...     # Fit the concept model\n...     nmf_explainer.fit(activations)\n...     concept_explainers.append(nmf_explainer)\n</code></pre> <pre><code>&gt;&gt;&gt; # Compute the stability metric\n&gt;&gt;&gt; stability = Stability(*concept_explainers)\n&gt;&gt;&gt; score = stability.compute()\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no <code>ConceptAutoEncoderExplainer</code>s or dictionary are provided.</p> <code>ValueError</code> <p>If the matching algorithm is not supported.</p> <code>ValueError</code> <p>If the dictionaries are not torch.Tensor.</p> <code>ValueError</code> <p>If the dictionaries have different shapes.</p> Source code in <code>interpreto/concepts/metrics/dictionary_metrics.py</code> <pre><code>def __init__(\n    self,\n    *concept_explainers: ConceptAutoEncoderExplainer | Float[torch.Tensor, \"cpt d\"],\n    matching_algorithm: DistanceFunctionProtocol = ConceptMatchingAlgorithm.COSINE_HUNGARIAN,\n):\n    if len(concept_explainers) &lt; 1:\n        raise ValueError(\"At least one `ConceptAutoEncoderExplainer`s or `torch.Tensor`s must be provided.\")\n\n    # if only one explainer is provided, duplicate it for self comparison\n    if len(concept_explainers) == 1:\n        concept_explainers = concept_explainers * 2\n\n    # extract dictionaries from concept explainers\n    self.dictionaries: list[Float[torch.Tensor, \"cpt d\"]] = [\n        ce.get_dictionary() if isinstance(ce, ConceptAutoEncoderExplainer) else ce for ce in concept_explainers\n    ]\n\n    expected_shape = None\n    for i, dictionary in enumerate(self.dictionaries):\n        if not isinstance(dictionary, torch.Tensor):\n            raise ValueError(\n                f\"Dictionary {i} or dictionary extracted from concept explainer {i} is not a torch.Tensor.\"\n            )\n\n        if len(dictionary.shape) != 2:\n            raise ValueError(\n                f\"Dictionary {i} or dictionary extracted from concept explainer {i} is not a 2D tensor.\"\n            )\n\n        expected_shape = dictionary.shape if expected_shape is None else expected_shape\n        if dictionary.shape != expected_shape:\n            raise ValueError(\n                f\"Dictionary {i} or dictionary extracted from concept explainer {i} has a different shape from the first dictionary.\"\n                f\"Expected shape: {expected_shape}, got shape: {dictionary.shape}.\"\n            )\n\n    self.distance_function = matching_algorithm\n</code></pre>"},{"location":"api/concepts/metrics/dictionary_metrics/#interpreto.concepts.metrics.Stability(concept_explainers)","title":"<code>concept_explainers</code>","text":""},{"location":"api/concepts/metrics/dictionary_metrics/#interpreto.concepts.metrics.Stability(matching_algorithm)","title":"<code>matching_algorithm</code>","text":""},{"location":"api/concepts/metrics/dictionary_metrics/#interpreto.concepts.metrics.Stability.compute","title":"compute","text":"<pre><code>compute()\n</code></pre> <p>Compute the mean score over pairwise comparison scores between dictionaries.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The stability score.</p> Source code in <code>interpreto/concepts/metrics/dictionary_metrics.py</code> <pre><code>def compute(self) -&gt; float:\n    \"\"\"Compute the mean score over pairwise comparison scores between dictionaries.\n\n    Returns:\n        float: The stability score.\n    \"\"\"\n    comparisons = []\n    for dict_1, dict_2 in combinations(self.dictionaries, 2):\n        # compute pairwise comparison\n        comparison = 1 - self.distance_function(dict_1, dict_2)\n        comparisons.append(comparison)\n\n    return torch.stack(comparisons).mean().item()\n</code></pre>"},{"location":"api/concepts/metrics/overview/","title":"Concept Explanation Metrics","text":"<p>As described in the Concept Explainers page, concept-based explanations are obtain by applying several steps:</p> <ul> <li> <p>Defining the concept-space, usually through dictionary learning.</p> </li> <li> <p>Interpreting the direction of the concept space.</p> </li> <li> <p>Eventually, measuring the importance of each concept in the prediction.</p> </li> </ul> <p>Concept-based metrics evaluate either one of the two first components, or both. To evaluate the importance of a concept, attribution metrics are used.</p> <p>Each metric takes in different arguments, hence no common API can be defined, apart from the <code>compute</code> method:</p> <pre><code>from interpreto.concepts.metrics import MetricClass\n\nmetric = MetricClass(...)\nscore = metric.compute(...)\n</code></pre> Metric family Property Metrics Dictionary Metrics Concept-space Stability Stability Reconstruction Metrics Concept-space Faithfulness MSE FID Custom Sparsity Metrics Concept-space Complexity Sparsity Sparsity Ratio"},{"location":"api/concepts/metrics/overview/#evaluating-the-concept-space","title":"Evaluating the concept-space","text":"<p>The concept space is define by a concept model encoding latent activations into concept activations. Some concept models can also reconstruct the latent activations from the concept activations.</p> <p>Several properties of the concept-space are desirable:</p> <ul> <li> <p>The concept-space should be faithful to the latent space data distribution.</p> </li> <li> <p>The concept-space should have a low complexity to push toward interpretability.</p> </li> <li> <p>The concept-space is stable across different training regimes.</p> </li> </ul>"},{"location":"api/concepts/metrics/overview/#concept-space-faithfulness","title":"Concept-space faithfulness","text":"<p>Concept-space faithfulness is often measured via the reconstruction error of the latent activations when projected back and forth in the concept space. Either in the latent space or in the logits space. The distance used to compare activations has a big impact on what is measured.</p> <p>In <code>interpreto</code> you can use the ReconstructionError to define a custom metric by specifying a <code>reconstruction_space</code> and a <code>distance_function</code>. Or you can use the MSE or FID metrics.</p>"},{"location":"api/concepts/metrics/overview/#concept-space-complexity","title":"Concept-space complexity","text":"<p>The concept-space complexity is often measured via the sparsity of its activations.</p> <p>In <code>interpreto</code> you can use: Sparsity and SparsityRatio.</p>"},{"location":"api/concepts/metrics/overview/#concept-space-stability","title":"Concept-space stability","text":"<p>The concept-space should stay the same across different training regimes. i.e. with different seeds, different data splits...</p> <p>In <code>interpreto</code> you can use: Stability to compare concept-model dictionaries.</p>"},{"location":"api/concepts/metrics/reconstruction_metrics/","title":"Concepts Reconstruction Metrics","text":"<p>Concept reconstruction error measures the faithfulness of the concept space with respect to the explained model latent space. To do so, these metrics compute the distance between initial activations and the reconstructed activations.</p> <pre><code>from interpreto.concepts.metrics import MetricClass\n\nmetric = MetricClass(concept_explainer)\nscore = metric.compute(activations)\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#mse","title":"MSE","text":""},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.MSE","title":"interpreto.concepts.metrics.MSE","text":"<pre><code>MSE(concept_explainer)\n</code></pre> <p>               Bases: <code>ReconstructionError</code></p> <p>Code  <code>concepts/metrics/reconstruction_metrics.py</code></p> <p>Evaluates wether the information reconstructed by the concept autoencoder corresponds to the original latent activations. It is a faithfulness metric. It is computed in the latent activations space through the Euclidean distance. It is also known as the reconstruction error.</p> <p>With \\(A\\) latent activations obtained through \\(A = h(X)\\), \\(t\\) and \\(t^{-1}\\) the concept encoder and decoders, the MSE is defined as: $$ \\sum_{a}^{A} ||t^{-1}(t(a)) - a||_2 $$</p> <p>TODO: make formula work</p> <p>Attributes:</p> Name Type Description <code>concept_explainer</code> <code>ConceptAutoEncoderExplainer</code> <p>The explainer used to compute concepts.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def __init__(\n    self,\n    concept_explainer: ConceptAutoEncoderExplainer,\n):\n    super().__init__(\n        concept_explainer=concept_explainer,\n        reconstruction_space=ReconstructionSpaces.LATENT_ACTIVATIONS,\n        distance_function=DistanceFunctions.EUCLIDEAN,\n    )\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.MSE.compute","title":"compute","text":"<pre><code>compute(latent_activations)\n</code></pre> <p>Compute the reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations | InterventionProxy</code> <p>The latent activations to use for the computation.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The reconstruction error.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def compute(self, latent_activations: LatentActivations | InterventionProxy) -&gt; float:\n    \"\"\"Compute the reconstruction error.\n\n    Args:\n        latent_activations (LatentActivations | InterventionProxy): The latent activations to use for the computation.\n\n    Returns:\n        float: The reconstruction error.\n    \"\"\"\n    split_latent_activations: LatentActivations = self.concept_explainer._sanitize_activations(latent_activations)\n\n    concepts_activations: ConceptsActivations = self.concept_explainer.encode_activations(split_latent_activations)\n\n    reconstructed_latent_activations: LatentActivations = self.concept_explainer.decode_concepts(\n        concepts_activations\n    )\n\n    if self.reconstruction_space is ReconstructionSpaces.LATENT_ACTIVATIONS:\n        return self.distance_function(split_latent_activations, reconstructed_latent_activations).item()\n\n    raise NotImplementedError(\"Only LATENT_ACTIVATIONS reconstruction space is supported.\")\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.MSE.compute(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/metrics/reconstruction_metrics/#fid","title":"FID","text":""},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.FID","title":"interpreto.concepts.metrics.FID","text":"<pre><code>FID(concept_explainer)\n</code></pre> <p>               Bases: <code>ReconstructionError</code></p> <p>Code  <code>concepts/metrics/reconstruction_metrics.py</code></p> <p>Evaluates wether the information reconstructed by the concept autoencoder corresponds to the original latent activations. It corresponds to a faithfulness metric, it measures if the reconstructed distribution matches the original distribution. It is computed in the latent activations space through the Wasserstein 1D distance.</p> <p>This metric was introduced by Fel et al. (2023)<sup>1</sup></p> <p>With \\(A\\) latent activations obtained through \\(A = h(X)\\), \\(t\\) and \\(t^{-1}\\) the concept encoder and decoders, and \\(\\mathcal{W}_1\\) the 1-Wassertein distance, the FID is defined as:</p> \\[ \\mathcal{W}_1(A, t^{-1}(t(A))) \\] <p>TODO: make formula work</p> <ol> <li> <p>Fel, T., Boutin, V., B\u00e9thune, L., Cad\u00e8ne, R., Moayeri, M., And\u00e9ol, L., Chavidal, M., &amp; Serre, T. A holistic approach to unifying automatic concept extraction and concept importance estimation. Advances in Neural Information Processing Systems. 2023.\u00a0\u21a9</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>concept_explainer</code> <code>ConceptAutoEncoderExplainer</code> <p>The explainer used to compute concepts.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def __init__(\n    self,\n    concept_explainer: ConceptAutoEncoderExplainer,\n):\n    super().__init__(\n        concept_explainer=concept_explainer,\n        reconstruction_space=ReconstructionSpaces.LATENT_ACTIVATIONS,\n        distance_function=DistanceFunctions.WASSERSTEIN_1D,\n    )\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.FID.compute","title":"compute","text":"<pre><code>compute(latent_activations)\n</code></pre> <p>Compute the reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations | InterventionProxy</code> <p>The latent activations to use for the computation.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The reconstruction error.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def compute(self, latent_activations: LatentActivations | InterventionProxy) -&gt; float:\n    \"\"\"Compute the reconstruction error.\n\n    Args:\n        latent_activations (LatentActivations | InterventionProxy): The latent activations to use for the computation.\n\n    Returns:\n        float: The reconstruction error.\n    \"\"\"\n    split_latent_activations: LatentActivations = self.concept_explainer._sanitize_activations(latent_activations)\n\n    concepts_activations: ConceptsActivations = self.concept_explainer.encode_activations(split_latent_activations)\n\n    reconstructed_latent_activations: LatentActivations = self.concept_explainer.decode_concepts(\n        concepts_activations\n    )\n\n    if self.reconstruction_space is ReconstructionSpaces.LATENT_ACTIVATIONS:\n        return self.distance_function(split_latent_activations, reconstructed_latent_activations).item()\n\n    raise NotImplementedError(\"Only LATENT_ACTIVATIONS reconstruction space is supported.\")\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.FID.compute(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/metrics/reconstruction_metrics/#custom","title":"Custom","text":"<p>To tune the reconstruction error to your need, you can specify a <code>reconstruction_space</code> and a <code>distance_function</code>, note that the <code>distance_function</code> should follow the <code>interpreto.commons.DistanceFunctionProtocol</code> protocol.</p>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.ReconstructionError","title":"interpreto.concepts.metrics.ReconstructionError","text":"<pre><code>ReconstructionError(concept_explainer, reconstruction_space, distance_function)\n</code></pre> <p>Code  <code>concepts/metrics/reconstruction_metrics.py</code></p> <p>Evaluates wether the information reconstructed by the concept autoencoder corresponds to the original latent activations. It corresponds to a faithfulness metric. The space where the distance thus error is computed and the distance function used can be specified.</p> <p>Attributes:</p> Name Type Description <code>concept_explainer</code> <code>ConceptAutoEncoderExplainer</code> <p>The explainer used to compute concepts.</p> <code>reconstruction_space</code> <code>ReconstructionSpaces</code> <p>The space in which the reconstruction error is computed.</p> <code>distance_function</code> <code>DistanceFunctionProtocol</code> <p>The distance function used to compute the reconstruction error.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def __init__(\n    self,\n    concept_explainer: ConceptAutoEncoderExplainer,\n    reconstruction_space: ReconstructionSpaces,\n    distance_function: DistanceFunctionProtocol,\n):\n    self.concept_explainer = concept_explainer\n    self.reconstruction_space = reconstruction_space\n    self.distance_function = distance_function\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.ReconstructionError.compute","title":"compute","text":"<pre><code>compute(latent_activations)\n</code></pre> <p>Compute the reconstruction error.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations | InterventionProxy</code> <p>The latent activations to use for the computation.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The reconstruction error.</p> Source code in <code>interpreto/concepts/metrics/reconstruction_metrics.py</code> <pre><code>def compute(self, latent_activations: LatentActivations | InterventionProxy) -&gt; float:\n    \"\"\"Compute the reconstruction error.\n\n    Args:\n        latent_activations (LatentActivations | InterventionProxy): The latent activations to use for the computation.\n\n    Returns:\n        float: The reconstruction error.\n    \"\"\"\n    split_latent_activations: LatentActivations = self.concept_explainer._sanitize_activations(latent_activations)\n\n    concepts_activations: ConceptsActivations = self.concept_explainer.encode_activations(split_latent_activations)\n\n    reconstructed_latent_activations: LatentActivations = self.concept_explainer.decode_concepts(\n        concepts_activations\n    )\n\n    if self.reconstruction_space is ReconstructionSpaces.LATENT_ACTIVATIONS:\n        return self.distance_function(split_latent_activations, reconstructed_latent_activations).item()\n\n    raise NotImplementedError(\"Only LATENT_ACTIVATIONS reconstruction space is supported.\")\n</code></pre>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.ReconstructionError.compute(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.concepts.metrics.ReconstructionSpaces","title":"interpreto.concepts.metrics.ReconstructionSpaces","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of possible reconstruction spaces. Latent activations go through the concept autoencoder to obtain reconstructed latent activations. Then it is possible to compute the distance between the original and reconstructed latent activations. First directly in the latent space, second in the logits space.</p> <p>Attributes:</p> Name Type Description <code>LATENT_ACTIVATIONS</code> <code>str</code> <p>Reconstruction space in the latent space.</p> <code>LOGITS</code> <code>str</code> <p>Reconstruction space in the logits space.</p>"},{"location":"api/concepts/metrics/reconstruction_metrics/#interpreto.commons.DistanceFunctions","title":"interpreto.commons.DistanceFunctions","text":"<p>               Bases: <code>Enum</code></p> <p>Enum of callable functions for computing distances between tensors.</p> Members <p>WASSERSTEIN_1D: Computes the 1D Wasserstein (earth mover's) distance between two tensors. EUCLIDEAN: Computes the Euclidean (L2) distance between two tensors. AVERAGE_EUCLIDEAN: Computes the average Euclidean distance between two tensors of samples. LP: Computes the Lp distance (generalization of Euclidean) between two tensors. AVERAGE_LP: Computes the average Lp distance between two tensors of samples. KL: Computes the Kullback-Leibler divergence between two tensors.</p>"},{"location":"api/concepts/metrics/sparsity_metrics/","title":"Concepts Sparsity Metrics","text":"<p>Concept sparsity metrics evaluate the sparsity of the concept-space activations. They take in the <code>concept_explainer</code> and the <code>latent_activations</code>, compute the <code>concept_activations</code> and then compute the sparsity of the <code>concept_activations</code>.</p> <pre><code>from interpreto.concepts.metrics import MetricClass\n\nmetric = MetricClass(concept_explainer)\nscore = metric.compute(activations)\n</code></pre>"},{"location":"api/concepts/metrics/sparsity_metrics/#sparsity","title":"Sparsity","text":""},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.Sparsity","title":"interpreto.concepts.metrics.Sparsity","text":"<pre><code>Sparsity(concept_explainer, epsilon=0.0)\n</code></pre> <p>Code  <code>concepts/metrics/sparsity_metrics.py</code></p> <p>Evaluates the sparsity of the concepts activations. It takes in the <code>concept_explainer</code> and the <code>latent_activations</code>, compute the <code>concept_activations</code> and then compute the sparsity of the <code>concept_activations</code>.</p> <p>The sparsity is defined as: $$ \\sum_{x}^{X} \\sum_{i=1}^{cpt} \\mathbb{1} ( | t(h(x))_i | &gt; \\epsilon ) $$ TODO: make the formula work</p> <p>Attributes:</p> Name Type Description <code>concept_explainer</code> <code>ConceptEncoderExplainer</code> <p>The explainer used to compute concepts.</p> <code>epsilon</code> <code>float</code> <p>The threshold used to compute the sparsity.</p> Source code in <code>interpreto/concepts/metrics/sparsity_metrics.py</code> <pre><code>def __init__(self, concept_explainer: ConceptEncoderExplainer, epsilon: float = 0.0):\n    self.concept_explainer = concept_explainer\n    self.epsilon = epsilon\n</code></pre>"},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.Sparsity.compute","title":"compute","text":"<pre><code>compute(latent_activations)\n</code></pre> <p>Compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations | InterventionProxy</code> <p>The latent activations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The metric.</p> Source code in <code>interpreto/concepts/metrics/sparsity_metrics.py</code> <pre><code>def compute(self, latent_activations: LatentActivations | InterventionProxy) -&gt; float:\n    \"\"\"Compute the metric.\n\n    Args:\n        latent_activations (LatentActivations | InterventionProxy): The latent activations.\n\n    Returns:\n        float: The metric.\n    \"\"\"\n    split_latent_activations: LatentActivations = self.concept_explainer._sanitize_activations(latent_activations)\n\n    concepts_activations: ConceptsActivations = self.concept_explainer.encode_activations(split_latent_activations)\n\n    return torch.mean(torch.abs(concepts_activations) &gt; self.epsilon, dtype=torch.float32).item()\n</code></pre>"},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.Sparsity.compute(latent_activations)","title":"<code>latent_activations</code>","text":""},{"location":"api/concepts/metrics/sparsity_metrics/#sparsity-ratio","title":"Sparsity Ratio","text":""},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.SparsityRatio","title":"interpreto.concepts.metrics.SparsityRatio","text":"<pre><code>SparsityRatio(concept_explainer, epsilon=0.0)\n</code></pre> <p>               Bases: <code>Sparsity</code></p> <p>Code  <code>concepts/metrics/sparsity_metrics.py</code></p> <p>Evaluates the sparsity ratio of the concepts activations. It takes in the <code>concept_explainer</code> and the <code>latent_activations</code>, compute the <code>concept_activations</code> and then compute the sparsity ratio of the <code>concept_activations</code>.</p> <p>With \\(A\\) latent activations obtained through \\(A = h(X)\\), the sparsity ratio is defined as: $$ (1 / cpt) * \\sum_{a}^{A} \\sum_{i=1}^{cpt} \\mathbb{1} ( | t(a)_i | &gt; \\epsilon ) $$ TODO: make the formula work</p> <p>Attributes:</p> Name Type Description <code>concept_explainer</code> <code>ConceptEncoderExplainer</code> <p>The explainer used to compute concepts.</p> <code>epsilon</code> <code>float</code> <p>The threshold used to compute the sparsity.</p> Source code in <code>interpreto/concepts/metrics/sparsity_metrics.py</code> <pre><code>def __init__(self, concept_explainer: ConceptEncoderExplainer, epsilon: float = 0.0):\n    self.concept_explainer = concept_explainer\n    self.epsilon = epsilon\n</code></pre>"},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.SparsityRatio.compute","title":"compute","text":"<pre><code>compute(latent_activations)\n</code></pre> <p>Compute the metric.</p> <p>Parameters:</p> Name Type Description Default <code>LatentActivations | InterventionProxy</code> <p>The latent activations.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The metric.</p> Source code in <code>interpreto/concepts/metrics/sparsity_metrics.py</code> <pre><code>def compute(self, latent_activations: LatentActivations | InterventionProxy) -&gt; float:\n    \"\"\"Compute the metric.\n\n    Args:\n        latent_activations (LatentActivations | InterventionProxy): The latent activations.\n\n    Returns:\n        float: The metric.\n    \"\"\"\n    sparsity = super().compute(latent_activations)\n    return sparsity / self.concept_explainer.concept_model.nb_concepts\n</code></pre>"},{"location":"api/concepts/metrics/sparsity_metrics/#interpreto.concepts.metrics.SparsityRatio.compute(latent_activations)","title":"<code>latent_activations</code>","text":""}]}